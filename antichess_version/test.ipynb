{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "New player has achieved a win rate of 1.0. It becomes the best player.\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/RL/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/chess_bot/antichess_version/test.ipynb Cell 1\u001b[0m line \u001b[0;36m<cell line: 134>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/antichess_version/test.ipynb#W0sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/antichess_version/test.ipynb#W0sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m     new_player_model \u001b[39m=\u001b[39m create_new_model()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/antichess_version/test.ipynb#W0sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m     best_player_model \u001b[39m=\u001b[39m train_new_player(best_player_model, new_player_model)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/antichess_version/test.ipynb#W0sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m     best_player_model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mbest_player.h5\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/chess_bot/antichess_version/test.ipynb Cell 1\u001b[0m line \u001b[0;36mtrain_new_player\u001b[0;34m(best_player_model, new_player_model, threshold_win_rate, num_games)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/antichess_version/test.ipynb#W0sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m         new_player_wins \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/antichess_version/test.ipynb#W0sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/antichess_version/test.ipynb#W0sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m     result \u001b[39m=\u001b[39m play_game(best_player_model, new_player_model)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/antichess_version/test.ipynb#W0sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0-1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/antichess_version/test.ipynb#W0sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m         new_player_wins \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;32m/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/chess_bot/antichess_version/test.ipynb Cell 1\u001b[0m line \u001b[0;36mplay_game\u001b[0;34m(model1, model2)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/antichess_version/test.ipynb#W0sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m board\u001b[39m.\u001b[39mis_game_over():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/antichess_version/test.ipynb#W0sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m     \u001b[39mif\u001b[39;00m board\u001b[39m.\u001b[39mturn \u001b[39m==\u001b[39m chess\u001b[39m.\u001b[39mWHITE:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/antichess_version/test.ipynb#W0sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m         move \u001b[39m=\u001b[39m choose_action(board, model1)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/antichess_version/test.ipynb#W0sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/antichess_version/test.ipynb#W0sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m         move \u001b[39m=\u001b[39m choose_action(board, model2)\n",
      "\u001b[1;32m/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/chess_bot/antichess_version/test.ipynb Cell 1\u001b[0m line \u001b[0;36mchoose_action\u001b[0;34m(board, model)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/antichess_version/test.ipynb#W0sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m legal_moves_list:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/antichess_version/test.ipynb#W0sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m chess\u001b[39m.\u001b[39mMove\u001b[39m.\u001b[39mnull()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/antichess_version/test.ipynb#W0sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m q_values \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(np\u001b[39m.\u001b[39;49marray([board_to_input_array(board)]))[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/antichess_version/test.ipynb#W0sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m best_move_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(q_values)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/antichess_version/test.ipynb#W0sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m best_move_uci \u001b[39m=\u001b[39m legal_moves_list[\u001b[39mmin\u001b[39m(best_move_index, \u001b[39mlen\u001b[39m(legal_moves_list)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)]\u001b[39m.\u001b[39muci()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/RL/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/RL/lib/python3.10/site-packages/keras/src/engine/training.py:2627\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2625\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution_tuner\u001b[39m.\u001b[39mstart()\n\u001b[1;32m   2626\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2627\u001b[0m \u001b[39mfor\u001b[39;00m _, iterator \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39menumerate_epochs():  \u001b[39m# Single epoch.\u001b[39;00m\n\u001b[1;32m   2628\u001b[0m     \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m   2629\u001b[0m         \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/RL/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:1341\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[39m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[0;32m-> 1341\u001b[0m     data_iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset)\n\u001b[1;32m   1342\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs):\n\u001b[1;32m   1343\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/RL/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:496\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mor\u001b[39;00m ops\u001b[39m.\u001b[39minside_function():\n\u001b[1;32m    495\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcolocate_with(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 496\u001b[0m     \u001b[39mreturn\u001b[39;00m iterator_ops\u001b[39m.\u001b[39;49mOwnedIterator(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    497\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39miteration in eager mode or within tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/RL/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:705\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    701\u001b[0m   \u001b[39mif\u001b[39;00m (components \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m element_spec \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    702\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    703\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    704\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnot be specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 705\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_iterator(dataset)\n\u001b[1;32m    707\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_next_call_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/RL/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:744\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    741\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(fulltype\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\n\u001b[1;32m    742\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_output_types)\n\u001b[1;32m    743\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator_resource\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[0;32m--> 744\u001b[0m gen_dataset_ops\u001b[39m.\u001b[39;49mmake_iterator(ds_variant, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/RL/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3420\u001b[0m, in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3418\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   3419\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3420\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   3421\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMakeIterator\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, dataset, iterator)\n\u001b[1;32m   3422\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   3423\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import chess\n",
    "import chess.variant\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Input\n",
    "from tensorflow.compat.v1.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from IPython.display import display, HTML\n",
    "import chess.svg\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from config import state_space_size, action_space_size, exploration_prob, learning_rate, discount_factor\n",
    "from board_function import board_to_input_array, state_to_index, move_to_output_array,  count_pieces_by_color, normalize_input\n",
    "\n",
    "# Chess Variant Antichess\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def choose_action(board, model):\n",
    "    if np.random.rand() < exploration_prob:\n",
    "        return np.random.choice(list(board.legal_moves))\n",
    "    else:\n",
    "        state_index = state_to_index(board)\n",
    "        legal_moves_list = list(board.legal_moves)\n",
    "        if not legal_moves_list:\n",
    "            return chess.Move.null()\n",
    "        q_values = model.predict(np.array([board_to_input_array(board)]))[0]\n",
    "        best_move_index = np.argmax(q_values)\n",
    "        best_move_uci = legal_moves_list[min(best_move_index, len(legal_moves_list)-1)].uci()\n",
    "        return chess.Move.from_uci(best_move_uci)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Neural Network Model alpha zero\n",
    "input_layer = Input(shape=state_space_size)\n",
    "conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)\n",
    "flatten_layer = Flatten()(conv2)\n",
    "dense1 = Dense(64, activation='relu')(flatten_layer)\n",
    "dense2 = Dense(64, activation='relu')(dense1)\n",
    "output_layer = Dense(action_space_size, activation='softmax')(dense2)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.1), loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "def update_q_table(state, action, reward, next_state):\n",
    "    state_index = state_to_index(state)\n",
    "    next_state_index = state_to_index(next_state)\n",
    "    action_index = list(state.legal_moves).index(action)\n",
    "\n",
    "    total_reward = reward\n",
    "    experience_replay_buffer.append((state_index, action_index, total_reward, next_state_index))\n",
    "    batch_size = min(len(experience_replay_buffer), 8)\n",
    "    if batch_size > 0:\n",
    "        batch = np.array(random.sample(experience_replay_buffer, batch_size))\n",
    "        states = np.array([board_to_input_array(chess.Board(fen=chess.STARTING_FEN)) for _ in batch[:, 0]])\n",
    "        next_states = np.array([board_to_input_array(chess.Board(fen=chess.STARTING_FEN)) for _ in batch[:, 3]])\n",
    "        q_values = model.predict(states)\n",
    "        next_q_values = model.predict(next_states)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            action_idx = int(batch[i, 1])\n",
    "            q_values[i, action_idx] += learning_rate * (batch[i, 2] + discount_factor * np.max(next_q_values[i]) - q_values[i, action_idx])\n",
    "        \n",
    "        model.train_on_batch(states, q_values)\n",
    "\n",
    "def calculate_reward(board):\n",
    "    reward = 0\n",
    "    piece_count = len(board.piece_map())\n",
    "    reward -= (32 - piece_count) * 0.1\n",
    "\n",
    "    if board.is_stalemate() or board.is_insufficient_material():\n",
    "        reward -= 5\n",
    "    elif board.is_fivefold_repetition() or board.is_seventyfive_moves():\n",
    "        reward -= 5\n",
    "    return reward\n",
    "\n",
    "def create_new_model():\n",
    "    new_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    new_model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.1), loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "    return new_model\n",
    "\n",
    "def train_model_self_play(num_games, model):\n",
    "    for _ in range(num_games):\n",
    "        play_game(model, model)\n",
    "\n",
    "def play_game(model1, model2):\n",
    "    board = chess.variant.GiveawayBoard()\n",
    "    while not board.is_game_over():\n",
    "        if board.turn == chess.WHITE:\n",
    "            move = choose_action(board, model1)\n",
    "        else:\n",
    "            move = choose_action(board, model2)\n",
    "        board.push(move)\n",
    "    return board.result()\n",
    "\n",
    "def train_new_player(best_player_model, new_player_model, threshold_win_rate=0.55, num_games=200):\n",
    "    new_player_wins = 0\n",
    "    for game in range(num_games):\n",
    "        if random.choice([True, False]):\n",
    "            result = play_game(new_player_model, best_player_model)\n",
    "            if result == \"1-0\":\n",
    "                new_player_wins += 1\n",
    "        else:\n",
    "            result = play_game(best_player_model, new_player_model)\n",
    "            if result == \"0-1\":\n",
    "                new_player_wins += 1\n",
    "\n",
    "        win_rate = new_player_wins / (game + 1)\n",
    "        if win_rate >= threshold_win_rate:\n",
    "            print(f\"New player has achieved a win rate of {win_rate}. It becomes the best player.\")\n",
    "            return new_player_model\n",
    "\n",
    "    print(f\"New player did not achieve the required win rate. Best player remains unchanged.\")\n",
    "    return best_player_model\n",
    "\n",
    "# Load or create initial best player model\n",
    "try:\n",
    "    best_player_model = load_model(\"best_player.h5\")\n",
    "except IOError:\n",
    "    print(\"No initial model found. Training a new model.\")\n",
    "    best_player_model = create_new_model()\n",
    "    train_model_self_play(200, best_player_model)\n",
    "\n",
    "# Main training and updating loop\n",
    "while True:\n",
    "    new_player_model = create_new_model()\n",
    "    best_player_model = train_new_player(best_player_model, new_player_model)\n",
    "    best_player_model.save(\"best_player.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
