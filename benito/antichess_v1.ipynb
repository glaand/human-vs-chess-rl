{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1681996823.py, line 210)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [7]\u001b[0;36m\u001b[0m\n\u001b[0;31m    elif board.is_stalemate() or board.is_insufficient_material():\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "from stockfish import Stockfish\n",
    "import chess\n",
    "import chess.variant\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from tensorflow.compat.v1.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from IPython.display import display, HTML\n",
    "import chess.svg\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Model\n",
    "import chess.pgn\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "#chess Variant Antichess\n",
    "\n",
    "\n",
    "\n",
    "def board_to_input_array(board):\n",
    "    board_array = np.zeros((8, 8, 12), dtype=np.uint8)\n",
    "    piece_mapping = {'r': 0, 'n': 1, 'b': 2, 'q': 3, 'k': 4, 'p': 5, 'R': 6, 'N': 7, 'B': 8, 'Q': 9, 'K': 10, 'P': 11}\n",
    "    #normalize piece values\n",
    "\n",
    "    \n",
    "    for square, piece in board.piece_map().items():\n",
    "        piece_type = piece_mapping[piece.symbol()]\n",
    "        color = int(piece.color)\n",
    "        board_array[square // 8, square % 8, piece_type] = color + 1  # Use 0 for empty squares\n",
    "\n",
    "    return board_array\n",
    "\n",
    "\n",
    "def state_to_index(board):\n",
    "    board_array = np.array(board_to_input_array(board))\n",
    "    return hash(board_array.tostring()) % state_space_size[0]\n",
    "\n",
    "\n",
    "\n",
    "def choose_action(board,model):\n",
    "    if np.random.rand() < exploration_prob:\n",
    "        return np.random.choice(list(board.legal_moves))\n",
    "    else:\n",
    "        state_index = state_to_index(board)\n",
    "        legal_moves_list = list(board.legal_moves)\n",
    "        if not legal_moves_list:\n",
    "            return chess.Move.null()\n",
    "        q_values = model.predict(np.array([board_to_input_array(board)]))[0]\n",
    "        best_move_index = np.argmax(q_values)\n",
    "        best_move_uci = legal_moves_list[min(best_move_index, len(legal_moves_list)-1)].uci()\n",
    "        return chess.Move.from_uci(best_move_uci)\n",
    "    \n",
    "# Function to convert a move into an output array\n",
    "def move_to_output_array(move, legal_moves):\n",
    "    output_array = np.zeros(action_space_size)\n",
    "    move_index = list(legal_moves).index(move)\n",
    "    output_array[move_index] = 1\n",
    "    return output_array\n",
    "\n",
    "# Path to Stockfish engine\n",
    "stockfish_path = \"/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/engine/stockfish\"\n",
    "\n",
    "stockfish = Stockfish(path=stockfish_path)\n",
    "\n",
    "#stockfish.set_skill_level(1)\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "discount_factor = 0.99\n",
    "exploration_prob = 0.2\n",
    "\n",
    "# Neural Network Architecture\n",
    "state_space_size = (8, 8, 12)  # 8x8 board with 12 channels (one for each piece type and color)\n",
    "action_space_size = 4096\n",
    "\n",
    "\n",
    "\n",
    "# Initialize a deque for experience replay\n",
    "experience_replay_buffer = deque(maxlen=1000)\n",
    "\n",
    "# Neural Network Model alpha zero\n",
    "\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=state_space_size)\n",
    "\n",
    "# Contracting path\n",
    "# Convolutional layers\n",
    "conv1 = Conv2D(128, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv1)\n",
    "flatten_layer = Flatten()(conv2)\n",
    "dense1 = Dense(128, activation='relu')(flatten_layer)\n",
    "dense2 = Dense(64, activation='relu')(dense1)\n",
    "output_layer = Dense(action_space_size, activation='softmax')(dense2)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.1), loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalize_input(board):\n",
    "    board_array = np.array(board_to_input_array(board), dtype=np.float16)\n",
    "    board_array /= 12.0  # Assuming the maximum piece type value is 12\n",
    "    return board_array\n",
    "\n",
    "\n",
    "\n",
    "def update_q_table(state, action, reward, next_state):\n",
    "    state_index = state_to_index(state)\n",
    "    next_state_index = state_to_index(next_state)\n",
    "    action_index = list(state.legal_moves).index(action)\n",
    "    \n",
    "\n",
    "\n",
    "    # Combine the rewards with weights (you can adjust the weights as needed)\n",
    "    #total_reward = reward + 0.01 * piece_coordination_reward_value\n",
    "    total_reward = reward\n",
    "\n",
    "    # Store the experience in the replay buffer\n",
    "    experience_replay_buffer.append((state_index, action_index, total_reward, next_state_index))\n",
    "\n",
    "    # Sample a batch from the replay buffer for training\n",
    "    batch_size = min(len(experience_replay_buffer), 8)\n",
    "    if batch_size > 0:\n",
    "        batch = np.array(random.sample(experience_replay_buffer, batch_size))\n",
    "        states = np.array([board_to_input_array(chess.Board(fen=chess.STARTING_FEN)) for _ in batch[:, 0]])\n",
    "        next_states = np.array([board_to_input_array(chess.Board(fen=chess.STARTING_FEN)) for _ in batch[:, 3]])\n",
    "        q_values = model.predict(states)\n",
    "        next_q_values = model.predict(next_states)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            action_idx = int(batch[i, 1])  # Cast to integer\n",
    "            q_values[i, action_idx] += learning_rate * (\n",
    "                batch[i, 2] + discount_factor * np.max(next_q_values[i]) - q_values[i, action_idx]\n",
    "            )\n",
    "        \n",
    "        # Train the model on the batch\n",
    "        model.train_on_batch(states, q_values)\n",
    "\n",
    "\n",
    "def display_chess_board(board):\n",
    "    return display(HTML(chess.svg.board(board=board, size=200)))\n",
    "\n",
    "def play_game():\n",
    "    board = chess.variant.AntichessBoard()\n",
    "    \n",
    "    game_states = []\n",
    "    total_reward = 0\n",
    "\n",
    "    while not board.is_game_over():\n",
    "        state = board.copy()\n",
    "        game_states.append(state.copy())\n",
    "\n",
    "        # Player 1 (White) makes a move\n",
    "        white_move = choose_action(board, model)\n",
    "        if white_move in board.legal_moves:\n",
    "            board.push(white_move)\n",
    "        else:\n",
    "            print(\"Invalid move by White. Try again.\")\n",
    "            continue\n",
    "\n",
    "        # Update state and check for game end\n",
    "        if board.is_game_over():\n",
    "            break\n",
    "\n",
    "        # Player 2 (Black) makes a move\n",
    "        black_move = choose_action(board, model)\n",
    "        if black_move in board.legal_moves:\n",
    "            board.push(black_move)\n",
    "        else:\n",
    "            print(\"Invalid move by Black. Try again.\")\n",
    "            continue\n",
    "\n",
    "        next_state = board.copy()\n",
    "\n",
    "        # Calculate rewards and update Q-table\n",
    "        reward = calculate_reward(board) # You need to define this function based on your reward strategy\n",
    "        update_q_table(state, white_move, reward, next_state)\n",
    "\n",
    "        # Accumulate the reward\n",
    "        total_reward += reward\n",
    "\n",
    "        # Next state becomes the current state for the next iteration\n",
    "        state = next_state\n",
    "\n",
    "    game_states.append(board.copy())\n",
    "    return game_states, board.result(), total_reward\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def calculate_reward(board):\n",
    "    reward = 0\n",
    "\n",
    "    # Reward for losing pieces\n",
    "    piece_count = len(board.piece_map())\n",
    "    reward -= (32 - piece_count) * 0.1  # Assuming a standard 32-piece setup\n",
    "\n",
    "\n",
    "    if board.is_stalemate() or board.is_insufficient_material():\n",
    "        # Penalize for drawing the game\n",
    "        reward -= 5\n",
    "    elif board.is_fivefold_repetition() or board.is_seventyfive_moves():\n",
    "        # Penalize for other types of draws\n",
    "        reward -= 5\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "log_dir = \"logs/\"\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "num_games = 100\n",
    "total_rewards = []\n",
    "results = {\"white_wins\": 0, \"black_wins\": 0, \"draws\": 0}\n",
    "outcomes = []\n",
    "\n",
    "\n",
    "reward_trend = []\n",
    "\n",
    "for episode in tqdm(range(num_games), desc=\"Training\"):\n",
    "    exploration_prob *= 0.99  # Decay exploration probability\n",
    "    print(\"Game:\", episode)\n",
    "    game_states, result, total_reward = play_game()\n",
    "\n",
    "    # Append the total reward to the reward trend list\n",
    "    reward_trend.append(total_reward)\n",
    "\n",
    "    # Update results based on the game outcome\n",
    "    if result == \"1-0\":\n",
    "        results[\"white_wins\"] += 1\n",
    "        outcomes.append(1)\n",
    "    elif result == \"0-1\":\n",
    "        results[\"black_wins\"] += 1\n",
    "        outcomes.append(0)\n",
    "    elif result == \"1/2-1/2\":\n",
    "        results[\"draws\"] += 1\n",
    "        outcomes.append(0.5)  # Fix here: Append 0.5 for draws\n",
    "\n",
    "    total_rewards.append(total_reward)\n",
    "    \n",
    "    # Display the total reward for each game\n",
    "    print(\"Total Reward for Game {}: {}\".format(episode, total_reward))\n",
    "    print(\"Game Outcome:\", result)\n",
    "    #game length\n",
    "    print(\"Game Length:\", len(game_states))\n",
    "\n",
    "# Display statistics\n",
    "average_reward = sum(total_rewards) / num_games\n",
    "print(\"Average Total Reward:\", average_reward)\n",
    "\n",
    "# Extract the FEN of the final position\n",
    "final_position_fen = game_states[-1].fen()\n",
    "print(\"Final Position FEN:\", final_position_fen)\n",
    "\n",
    "# Display the last game\n",
    "for state in game_states:\n",
    "    display_chess_board(state)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nResults Summary:\")\n",
    "print(\"White Wins:\", results[\"white_wins\"])\n",
    "print(\"Black Wins:\", results[\"black_wins\"])\n",
    "print(\"Draws:\", results[\"draws\"])\n",
    "\n",
    "# Plot trend lines\n",
    "plt.plot(outcomes, label=\"Game Outcomes\")\n",
    "plt.xlabel(\"Games\")\n",
    "plt.ylabel(\"Outcome (1 for White Win, 0 for Draw, 0.5 for Loss)\")\n",
    "plt.legend()\n",
    "plt.title(\"Game Outcomes Trend\")\n",
    "\n",
    "# Plot rewards\n",
    "plt.figure()\n",
    "plt.plot(reward_trend, label=\"Rewards\")\n",
    "plt.xlabel(\"Games\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.legend()\n",
    "plt.title(\"Reward Trend\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "\n",
    "#model.save(\"carlsen_stock_trained.h5\")  # Change the file name as needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
