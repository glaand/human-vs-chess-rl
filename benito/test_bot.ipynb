{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chess game against Stockfish\n",
      "\n",
      "r n b q k b n r\n",
      "p p p p p p p p\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      "P P P P P P P P\n",
      "R N B Q K B N R\n"
     ]
    },
    {
     "ename": "InvalidMoveError",
     "evalue": "expected uci string to be of length 4 or 5: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidMoveError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/self-driving-car/benito/test_bot.ipynb Cell 1\u001b[0m line \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/self-driving-car/benito/test_bot.ipynb#W0sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mResult:\u001b[39m\u001b[39m\"\u001b[39m, board\u001b[39m.\u001b[39mresult())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/self-driving-car/benito/test_bot.ipynb#W0sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# Play the game\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/self-driving-car/benito/test_bot.ipynb#W0sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m play_game()\n",
      "\u001b[1;32m/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/self-driving-car/benito/test_bot.ipynb Cell 1\u001b[0m line \u001b[0;36mplay_game\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/self-driving-car/benito/test_bot.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Player's move\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/self-driving-car/benito/test_bot.ipynb#W0sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m player_move \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mYour move (in algebraic notation): \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/self-driving-car/benito/test_bot.ipynb#W0sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mif\u001b[39;00m chess\u001b[39m.\u001b[39;49mMove\u001b[39m.\u001b[39;49mfrom_uci(player_move) \u001b[39min\u001b[39;00m board\u001b[39m.\u001b[39mlegal_moves:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/self-driving-car/benito/test_bot.ipynb#W0sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     board\u001b[39m.\u001b[39mpush_uci(player_move)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/self-driving-car/benito/test_bot.ipynb#W0sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/RL/lib/python3.10/site-packages/chess/__init__.py:612\u001b[0m, in \u001b[0;36mMove.from_uci\u001b[0;34m(cls, uci)\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(from_square, to_square, promotion\u001b[39m=\u001b[39mpromotion)\n\u001b[1;32m    611\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 612\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidMoveError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected uci string to be of length 4 or 5: \u001b[39m\u001b[39m{\u001b[39;00muci\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mInvalidMoveError\u001b[0m: expected uci string to be of length 4 or 5: ''"
     ]
    }
   ],
   "source": [
    "from stockfish import Stockfish\n",
    "import chess\n",
    "\n",
    "stockfish_path = \"/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/engine/stockfish\"\n",
    "stockfish = Stockfish(path=stockfish_path)\n",
    "\n",
    "def print_board(board):\n",
    "    print(board)\n",
    "\n",
    "def play_game():\n",
    "    board = chess.Board()\n",
    "\n",
    "    print(\"Chess game against Stockfish\\n\")\n",
    "\n",
    "    while not board.is_game_over():\n",
    "        print_board(board)\n",
    "\n",
    "        # Player's move\n",
    "        player_move = input(\"Your move (in algebraic notation): \")\n",
    "        if chess.Move.from_uci(player_move) in board.legal_moves:\n",
    "            board.push_uci(player_move)\n",
    "        else:\n",
    "            print(\"Invalid move. Try again.\")\n",
    "            continue\n",
    "\n",
    "        if board.is_game_over():\n",
    "            break\n",
    "\n",
    "        # Stockfish's move\n",
    "        stockfish.set_fen_position(board.fen())\n",
    "        stockfish_move = stockfish.get_best_move()\n",
    "        print(\"Stockfish's move:\", stockfish_move)\n",
    "        board.push_uci(stockfish_move)\n",
    "\n",
    "    print(\"\\nGame Over\")\n",
    "    print(\"Result:\", board.result())\n",
    "\n",
    "# Play the game\n",
    "play_game()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 14\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 16\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 17\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 21\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 19\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 11\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 10\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 20\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 20\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 21\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 14\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 11\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 8\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 12\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 16\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 8\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 11\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 15\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 11\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 20\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 16\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 18\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 13\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 13\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 19\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 8\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 9\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 14\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 13\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 17\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 13\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 20\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 14\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 9\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 13\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 16\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 14\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 11\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 9\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 14\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 11\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 16\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 4\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 10\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 8\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 18\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 16\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 15\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 9\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 14\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 16\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 10\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 10\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 10\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 24\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 6\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 12\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 9\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 19\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 12\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 19\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 18\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 16\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 19\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 16\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 20\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 14\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 7\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 22\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 20\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 23\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 19\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 7\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 18\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 12\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 8\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 19\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 11\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 11\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 9\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 14\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 16\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 16\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 17\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 22\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 18\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 16\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 10\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 13\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 14\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 18\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 11\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 23\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 6\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 10\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 5\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 14\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 10\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 14\n",
      "\n",
      "Game Over\n",
      "Result: 0-1\n",
      "Number of moves: 7\n"
     ]
    }
   ],
   "source": [
    "from stockfish import Stockfish\n",
    "import chess\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.compat.v1.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "stockfish_path = \"/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/engine/stockfish\"\n",
    "stockfish = Stockfish(path=stockfish_path)\n",
    "\n",
    "# Define Q-learning parameters\n",
    "learning_rate = 0.001\n",
    "discount_factor = 0.9\n",
    "exploration_prob = 0.2\n",
    "\n",
    "# Initialize the Q-table\n",
    "state_space_size = 64  # Assuming a simplified state space\n",
    "action_space_size = 4096  # Number of possible chess moves\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "\n",
    "# Create a Q-network\n",
    "model = Sequential([\n",
    "    Dense(64, input_shape=(state_space_size,), activation='relu'),\n",
    "    Dense(action_space_size, activation='linear')\n",
    "])\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "\n",
    "def state_to_index(board):\n",
    "    # Convert the board state to a unique index for the Q-table\n",
    "    return hash(board.fen()) % state_space_size\n",
    "\n",
    "def choose_action(board):\n",
    "    # Epsilon-greedy policy for action selection\n",
    "    if np.random.rand() < exploration_prob:\n",
    "        return np.random.choice(list(board.legal_moves))\n",
    "    else:\n",
    "        state_index = state_to_index(board)\n",
    "        legal_moves_list = list(board.legal_moves)\n",
    "        if not legal_moves_list:\n",
    "            return chess.Move.null()  # No legal moves, return a null move\n",
    "        best_move_index = np.argmax(q_table[state_index])\n",
    "        best_move_uci = legal_moves_list[min(best_move_index, len(legal_moves_list)-1)].uci()\n",
    "        return chess.Move.from_uci(best_move_uci)\n",
    "\n",
    "def update_q_table(state, action, reward, next_state):\n",
    "    state_index = state_to_index(state)\n",
    "    next_state_index = state_to_index(next_state)\n",
    "\n",
    "    action_index = list(state.legal_moves).index(action)\n",
    "\n",
    "    best_next_action = np.argmax(q_table[next_state_index])\n",
    "\n",
    "    q_table[state_index, action_index] += learning_rate * (\n",
    "        reward + discount_factor * q_table[next_state_index, best_next_action] - q_table[state_index, action_index]\n",
    "    )\n",
    "\n",
    "def print_board(board):\n",
    "    print(board)\n",
    "\n",
    "def play_game():\n",
    "    board = chess.Board()\n",
    "\n",
    "    while not board.is_game_over():\n",
    "        state = board.copy()\n",
    "\n",
    "        #print(\"Number of moves:\", board.fullmove_number)\n",
    "\n",
    "        rl_move = choose_action(board)\n",
    "        if rl_move in board.legal_moves:\n",
    "            board.push(rl_move)\n",
    "        else:\n",
    "            print(\"Invalid move. Try again.\")\n",
    "            continue\n",
    "\n",
    "        if board.is_game_over():\n",
    "            break\n",
    "\n",
    "        stockfish.set_fen_position(board.fen())\n",
    "        stockfish_move_uci = stockfish.get_best_move()\n",
    "        #print(\"Stockfish's move:\", stockfish_move_uci)\n",
    "        stockfish_move = chess.Move.from_uci(stockfish_move_uci)\n",
    "        next_state = board.copy()\n",
    "        board.push(stockfish_move)\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        if board.result() == \"1-0\":\n",
    "            reward = 1e6\n",
    "        elif board.result() == \"0-1\":\n",
    "            reward = -1e6\n",
    "        elif board.result() == \"1/2-1/2\":\n",
    "            reward = 0.1\n",
    "        elif board.is_capture(rl_move):\n",
    "            reward = 0.01\n",
    "        elif board.is_capture(stockfish_move):\n",
    "            reward = -0.02\n",
    "\n",
    "        update_q_table(state, rl_move, reward, next_state)\n",
    "\n",
    "    print(\"\\nGame Over\")\n",
    "    print(\"Result:\", board.result())\n",
    "    #print number of moves the game had\n",
    "    print(\"Number of moves:\", board.fullmove_number)\n",
    "\n",
    "# Play multiple games\n",
    "log_dir = \"logs/\"\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "num_games = 100\n",
    "for episode in range(num_games):\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    play_game()\n",
    "\n",
    "    with tf.summary.create_file_writer(log_dir).as_default():\n",
    "        tf.summary.scalar('Total Reward', total_reward, step=episode)\n",
    "        tf.summary.scalar('Steps', steps, step=episode)\n",
    "        tf.summary.flush()\n",
    "\n",
    "        for layer in model.layers:\n",
    "            for weight in layer.weights:\n",
    "                tf.summary.histogram(weight.name, weight, step=episode)\n",
    "\n",
    "# model.save(\"trained_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of moves: 1\n",
      "Stockfish's move: d7d5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1 into shape (1,64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/self-driving-car/benito/test_bot.ipynb Cell 3\u001b[0m line \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/self-driving-car/benito/test_bot.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m total_reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/self-driving-car/benito/test_bot.ipynb#W1sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m steps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/self-driving-car/benito/test_bot.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m play_game()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/self-driving-car/benito/test_bot.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Log metrics to TensorBoard after each episode\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/self-driving-car/benito/test_bot.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mcreate_file_writer(log_dir)\u001b[39m.\u001b[39mas_default():\n",
      "\u001b[1;32m/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/self-driving-car/benito/test_bot.ipynb Cell 3\u001b[0m line \u001b[0;36mplay_game\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/self-driving-car/benito/test_bot.ipynb#W1sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m \u001b[39melif\u001b[39;00m board\u001b[39m.\u001b[39mis_capture(stockfish_move):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/self-driving-car/benito/test_bot.ipynb#W1sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m     \u001b[39m# Your own piece captured\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/self-driving-car/benito/test_bot.ipynb#W1sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m0.02\u001b[39m  \u001b[39m# Very small negative reward for your piece getting captured\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/self-driving-car/benito/test_bot.ipynb#W1sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m update_q_table(state, rl_move, reward, next_state)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/self-driving-car/benito/test_bot.ipynb#W1sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mGame Over\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/self-driving-car/benito/test_bot.ipynb#W1sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mResult:\u001b[39m\u001b[39m\"\u001b[39m, board\u001b[39m.\u001b[39mresult())\n",
      "\u001b[1;32m/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/self-driving-car/benito/test_bot.ipynb Cell 3\u001b[0m line \u001b[0;36mupdate_q_table\u001b[0;34m(state, action, reward, next_state)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/self-driving-car/benito/test_bot.ipynb#W1sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_q_table\u001b[39m(state, action, reward, next_state):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/self-driving-car/benito/test_bot.ipynb#W1sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mreshape(state, (\u001b[39m1\u001b[39;49m, state_space_size))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/self-driving-car/benito/test_bot.ipynb#W1sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     next_state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(next_state, (\u001b[39m1\u001b[39m, state_space_size))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/self-driving-car/benito/test_bot.ipynb#W1sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39m# Predict Q-values for the current state and the next state\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/RL/lib/python3.10/site-packages/numpy/core/fromnumeric.py:285\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[1;32m    201\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreshape\u001b[39m(a, newshape, order\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    202\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[39m           [5, 6]])\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39mreshape\u001b[39;49m\u001b[39m'\u001b[39;49m, newshape, order\u001b[39m=\u001b[39;49morder)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/RL/lib/python3.10/site-packages/numpy/core/fromnumeric.py:56\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m bound \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, method, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m \u001b[39mif\u001b[39;00m bound \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/RL/lib/python3.10/site-packages/numpy/core/fromnumeric.py:45\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     wrap \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(asarray(obj), method)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m wrap:\n\u001b[1;32m     47\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, mu\u001b[39m.\u001b[39mndarray):\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1 into shape (1,64)"
     ]
    }
   ],
   "source": [
    "log_dir = \"logs/\"\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Create a model (you may need a more complex model based on your problem)\n",
    "\n",
    "# Play multiple games\n",
    "num_games = 10\n",
    "for episode in range(num_games):\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    play_game()\n",
    "\n",
    "    # Log metrics to TensorBoard after each episode\n",
    "    with tf.summary.create_file_writer(log_dir).as_default():\n",
    "        tf.summary.scalar('Total Reward', total_reward, step=episode)\n",
    "        tf.summary.scalar('Steps', steps, step=episode)\n",
    "        tf.summary.flush() \n",
    "\n",
    "        # Optionally, log additional metrics such as the model's weights or gradients\n",
    "        for layer in model.layers:\n",
    "            for weight in layer.weights:\n",
    "                tf.summary.histogram(weight.name, weight, step=episode)\n",
    "\n",
    "# model.save(\"trained_model.h5\")\n",
    "\n",
    "\n",
    "\n",
    "#model.save(\"trained_model.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
