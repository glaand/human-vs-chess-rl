{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stockfish import Stockfish\n",
    "import chess\n",
    "\n",
    "stockfish_path = \"/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/engine/stockfish\"\n",
    "stockfish = Stockfish(path=stockfish_path)\n",
    "\n",
    "def print_board(board):\n",
    "    print(board)\n",
    "\n",
    "def play_game():\n",
    "    board = chess.Board()\n",
    "\n",
    "    print(\"Chess game against Stockfish\\n\")\n",
    "\n",
    "    while not board.is_game_over():\n",
    "        print_board(board)\n",
    "\n",
    "        # Player's move\n",
    "        player_move = input(\"Your move (in algebraic notation): \")\n",
    "        if chess.Move.from_uci(player_move) in board.legal_moves:\n",
    "            board.push_uci(player_move)\n",
    "        else:\n",
    "            print(\"Invalid move. Try again.\")\n",
    "            continue\n",
    "\n",
    "        if board.is_game_over():\n",
    "            break\n",
    "\n",
    "        # Stockfish's move\n",
    "        stockfish.set_fen_position(board.fen())\n",
    "        stockfish_move = stockfish.get_best_move()\n",
    "        print(\"Stockfish's move:\", stockfish_move)\n",
    "        board.push_uci(stockfish_move)\n",
    "\n",
    "    print(\"\\nGame Over\")\n",
    "    print(\"Result:\", board.result())\n",
    "\n",
    "# Play the game\n",
    "#play_game()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game: 0\n",
      "Game: 1\n",
      "Game: 2\n",
      "Game: 3\n",
      "Game: 4\n",
      "Game: 5\n",
      "Game: 6\n",
      "Game: 7\n"
     ]
    }
   ],
   "source": [
    "from stockfish import Stockfish\n",
    "import chess\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.compat.v1.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from IPython.display import display, HTML\n",
    "import chess.svg\n",
    "\n",
    "stockfish_path = \"/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/engine/stockfish\"\n",
    "stockfish = Stockfish(path=stockfish_path)\n",
    "\n",
    "learning_rate = 0.001\n",
    "discount_factor = 0.9\n",
    "exploration_prob = 0.2\n",
    "\n",
    "state_space_size = 64\n",
    "action_space_size = 4096\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, input_shape=(state_space_size,), activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(action_space_size, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "\n",
    "def state_to_index(board):\n",
    "    return hash(board.fen()) % state_space_size\n",
    "\n",
    "def choose_action(board):\n",
    "    if np.random.rand() < exploration_prob:\n",
    "        return np.random.choice(list(board.legal_moves))\n",
    "    else:\n",
    "        state_index = state_to_index(board)\n",
    "        legal_moves_list = list(board.legal_moves)\n",
    "        if not legal_moves_list:\n",
    "            return chess.Move.null()\n",
    "        best_move_index = np.argmax(q_table[state_index])\n",
    "        best_move_uci = legal_moves_list[min(best_move_index, len(legal_moves_list)-1)].uci()\n",
    "        return chess.Move.from_uci(best_move_uci)\n",
    "\n",
    "def update_q_table(state, action, reward, next_state):\n",
    "    state_index = state_to_index(state)\n",
    "    next_state_index = state_to_index(next_state)\n",
    "    action_index = list(state.legal_moves).index(action)\n",
    "    best_next_action = np.argmax(q_table[next_state_index])\n",
    "    q_table[state_index, action_index] += learning_rate * (\n",
    "        reward + discount_factor * q_table[next_state_index, best_next_action] - q_table[state_index, action_index]\n",
    "    )\n",
    "\n",
    "def display_chess_board(board):\n",
    "    return display(HTML(chess.svg.board(board=board, size=400)))\n",
    "\n",
    "def play_game():\n",
    "    board = chess.Board()\n",
    "    game_states = []\n",
    "\n",
    "    while not board.is_game_over():\n",
    "        state = board.copy()\n",
    "        game_states.append(state.copy())\n",
    "\n",
    "        rl_move = choose_action(board)\n",
    "        if rl_move in board.legal_moves:\n",
    "            board.push(rl_move)\n",
    "        else:\n",
    "            print(\"Invalid move. Try again.\")\n",
    "            continue\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        if board.is_game_over():\n",
    "            break\n",
    "\n",
    "        stockfish.set_fen_position(board.fen())\n",
    "        stockfish_move_uci = stockfish.get_best_move()\n",
    "        stockfish_move = chess.Move.from_uci(stockfish_move_uci)\n",
    "        next_state = board.copy()\n",
    "        board.push(stockfish_move)\n",
    "\n",
    "        if next_state.is_check():\n",
    "            reward = 0.5\n",
    "\n",
    "        if board.result() == \"1-0\":\n",
    "            reward = 1e6\n",
    "        elif board.result() == \"0-1\":\n",
    "            reward = -1e6\n",
    "        elif board.result() == \"1/2-1/2\":\n",
    "            reward = 0.1\n",
    "        elif board.is_capture(rl_move):\n",
    "            reward = 1\n",
    "        elif board.is_capture(stockfish_move):\n",
    "            reward = -1\n",
    "\n",
    "        update_q_table(state, rl_move, reward, next_state)\n",
    "\n",
    "    game_states.append(board.copy())\n",
    "    return game_states, board.result()\n",
    "\n",
    "log_dir = \"logs/\"\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "num_games = 10\n",
    "for episode in range(num_games):\n",
    "    print(\"Game:\", episode)\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    game_states, result = play_game()\n",
    "\n",
    "    with tf.summary.create_file_writer(log_dir).as_default():\n",
    "        tf.summary.scalar('Total Reward', total_reward, step=episode)\n",
    "        tf.summary.scalar('Steps', steps, step=episode)\n",
    "        tf.summary.flush()\n",
    "\n",
    "        for layer in model.layers:\n",
    "            for weight in layer.weights:\n",
    "                tf.summary.histogram(weight.name, weight, step=episode)\n",
    "\n",
    "    if episode == num_games - 1:\n",
    "        for state in game_states:\n",
    "            display_chess_board(state)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
