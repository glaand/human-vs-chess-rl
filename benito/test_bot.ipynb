{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "from stockfish import Stockfish\n",
    "import chess\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from tensorflow.compat.v1.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from IPython.display import display, HTML\n",
    "import chess.svg\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Model\n",
    "import chess.pgn\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pgn_data(pgn_file_path):\n",
    "    print(\"Loading PGN data from {}...\",pgn_file_path)\n",
    "    pgn_data = []\n",
    "    with open(pgn_file_path) as pgn:\n",
    "        while True:\n",
    "            game = chess.pgn.read_game(pgn)\n",
    "            if game is None:\n",
    "                break\n",
    "            board = game.board()\n",
    "            for move in game.mainline_moves():\n",
    "                input_array = board_to_input_array(board)\n",
    "                output_array = move_to_output_array(move, board.legal_moves)\n",
    "                pgn_data.append((input_array, output_array))\n",
    "                board.push(move)\n",
    "    return pgn_data\n",
    "\n",
    "\n",
    "def board_to_input_array(board):\n",
    "    board_array = np.zeros((8, 8, 12), dtype=np.uint8)\n",
    "    piece_mapping = {'r': 0, 'n': 1, 'b': 2, 'q': 3, 'k': 4, 'p': 5, 'R': 6, 'N': 7, 'B': 8, 'Q': 9, 'K': 10, 'P': 11}\n",
    "    #normalize piece values\n",
    "\n",
    "    \n",
    "    for square, piece in board.piece_map().items():\n",
    "        piece_type = piece_mapping[piece.symbol()]\n",
    "        color = int(piece.color)\n",
    "        board_array[square // 8, square % 8, piece_type] = color + 1  # Use 0 for empty squares\n",
    "\n",
    "    return board_array\n",
    "\n",
    "\n",
    "def state_to_index(board):\n",
    "    board_array = np.array(board_to_input_array(board))\n",
    "    return hash(board_array.tostring()) % state_space_size[0]\n",
    "\n",
    "\n",
    "\n",
    "def choose_action(board,model):\n",
    "    if np.random.rand() < exploration_prob:\n",
    "        return np.random.choice(list(board.legal_moves))\n",
    "    else:\n",
    "        state_index = state_to_index(board)\n",
    "        legal_moves_list = list(board.legal_moves)\n",
    "        if not legal_moves_list:\n",
    "            return chess.Move.null()\n",
    "        q_values = model.predict(np.array([board_to_input_array(board)]))[0]\n",
    "        best_move_index = np.argmax(q_values)\n",
    "        best_move_uci = legal_moves_list[min(best_move_index, len(legal_moves_list)-1)].uci()\n",
    "        return chess.Move.from_uci(best_move_uci)\n",
    "    \n",
    "# Function to convert a move into an output array\n",
    "def move_to_output_array(move, legal_moves):\n",
    "    output_array = np.zeros(action_space_size)\n",
    "    move_index = list(legal_moves).index(move)\n",
    "    output_array[move_index] = 1\n",
    "    return output_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PGN data from {}... /Users/benitorusconi/Downloads/Carlsen_plus-stockfish.pgn\n"
     ]
    }
   ],
   "source": [
    "# Path to Stockfish engine\n",
    "stockfish_path = \"/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/engine/stockfish\"\n",
    "pgn_file_path = '/Users/benitorusconi/Downloads/Carlsen_plus-stockfish.pgn'\n",
    "stockfish = Stockfish(path=stockfish_path,)\n",
    "\n",
    "#stockfish.set_skill_level(1)\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "exploration_prob = 0.2\n",
    "\n",
    "# Neural Network Architecture\n",
    "state_space_size = (8, 8, 12)  # 8x8 board with 12 channels (one for each piece type and color)\n",
    "action_space_size = 4096\n",
    "\n",
    "\n",
    "\n",
    "# Initialize a deque for experience replay\n",
    "experience_replay_buffer = deque(maxlen=1000)\n",
    "\n",
    "# Neural Network Model alpha zero\n",
    "\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=state_space_size)\n",
    "\n",
    "# Contracting path\n",
    "# Convolutional layers\n",
    "conv1 = Conv2D(128, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv1)\n",
    "flatten_layer = Flatten()(conv2)\n",
    "dense1 = Dense(128, activation='relu')(flatten_layer)\n",
    "dense2 = Dense(64, activation='relu')(dense1)\n",
    "output_layer = Dense(action_space_size, activation='softmax')(dense2)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.1), loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Pre-training the model on PGN games\n",
    "def pretrain_model(model, pgn_data):\n",
    "    print(\"Pre-training on PGN data...\")\n",
    "    total_pgn_games = len(pgn_data)\n",
    "\n",
    "    # Use tqdm to create a progress bar\n",
    "    for game_idx, (input_array, output_array) in enumerate(tqdm(pgn_data, desc=\"Processing\", ncols=100), start=1):\n",
    "        # Reshape input for the model, if necessary\n",
    "        input_array = input_array.reshape((1,) + input_array.shape)\n",
    "        output_array = output_array.reshape((1,) + output_array.shape)\n",
    "        model.train_on_batch(input_array, output_array)\n",
    "\n",
    "    # Save the pretrained model\n",
    "    model.save(\"carlse_stockfish_giucio.h5\")  # Change the file name as needed\n",
    "    print(\"Pretrained model saved.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalize_input(board):\n",
    "    board_array = np.array(board_to_input_array(board), dtype=np.float16)\n",
    "    board_array /= 12.0  # Assuming the maximum piece type value is 12\n",
    "    return board_array\n",
    "\n",
    "\n",
    "\n",
    "def update_q_table(state, action, reward, next_state):\n",
    "    state_index = state_to_index(state)\n",
    "    next_state_index = state_to_index(next_state)\n",
    "    action_index = list(state.legal_moves).index(action)\n",
    "    \n",
    "\n",
    "\n",
    "    # Combine the rewards with weights (you can adjust the weights as needed)\n",
    "    #total_reward = reward + 0.01 * piece_coordination_reward_value\n",
    "    total_reward = reward\n",
    "\n",
    "    # Store the experience in the replay buffer\n",
    "    experience_replay_buffer.append((state_index, action_index, total_reward, next_state_index))\n",
    "\n",
    "    # Sample a batch from the replay buffer for training\n",
    "    batch_size = min(len(experience_replay_buffer), 8)\n",
    "    if batch_size > 0:\n",
    "        batch = np.array(random.sample(experience_replay_buffer, batch_size))\n",
    "        states = np.array([board_to_input_array(chess.Board(fen=chess.STARTING_FEN)) for _ in batch[:, 0]])\n",
    "        next_states = np.array([board_to_input_array(chess.Board(fen=chess.STARTING_FEN)) for _ in batch[:, 3]])\n",
    "        q_values = model.predict(states)\n",
    "        next_q_values = model.predict(next_states)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            action_idx = int(batch[i, 1])  # Cast to integer\n",
    "            q_values[i, action_idx] += learning_rate * (\n",
    "                batch[i, 2] + discount_factor * np.max(next_q_values[i]) - q_values[i, action_idx]\n",
    "            )\n",
    "        \n",
    "        # Train the model on the batch\n",
    "        model.train_on_batch(states, q_values)\n",
    "\n",
    "\n",
    "def display_chess_board(board):\n",
    "    return display(HTML(chess.svg.board(board=board, size=200)))\n",
    "\n",
    "def play_game():\n",
    "    \n",
    "    \n",
    "    board = chess.Board()\n",
    "\n",
    "    \n",
    "    game_states = []\n",
    "    total_reward = 0  # Initialize total_reward\n",
    "\n",
    "    while not board.is_game_over():\n",
    "        state = board.copy()\n",
    "        game_states.append(state.copy())\n",
    "\n",
    "        rl_move = choose_action(board,model)\n",
    "        if rl_move in board.legal_moves:\n",
    "            board.push(rl_move)\n",
    "        else:\n",
    "            print(\"Invalid move. Try again.\")\n",
    "            continue\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        if board.is_game_over():\n",
    "            break\n",
    "\n",
    "        stockfish.set_fen_position(board.fen())\n",
    "        stockfish_move_uci = stockfish.get_best_move()\n",
    "        stockfish_move = chess.Move.from_uci(stockfish_move_uci)\n",
    "        next_state = board.copy()\n",
    "        board.push(stockfish_move)\n",
    "\n",
    "        if next_state.is_check():\n",
    "            reward = 0.1\n",
    "\n",
    "        if board.result() == \"1-0\":\n",
    "            reward += 100  # Win\n",
    "        elif board.result() == \"0-1\":\n",
    "            move_number = len(game_states)\n",
    "            reward -=100 # Loss \n",
    "        elif board.result() == \"1/2-1/2\":\n",
    "            reward -= 100  # Draw\n",
    "            \n",
    "            \n",
    "        #capture reward\n",
    "        \n",
    "        if board.is_capture(rl_move):\n",
    "            reward += piece_value(board.piece_at(rl_move.to_square))\n",
    "            \n",
    "        if board.is_capture(stockfish_move):\n",
    "            reward -= piece_value(board.piece_at(stockfish_move.to_square))\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        update_q_table(state, rl_move, reward, next_state)\n",
    "\n",
    "        # Accumulate the reward\n",
    "        total_reward += reward\n",
    "\n",
    "\n",
    "\n",
    "    game_states.append(board.copy())\n",
    "    return game_states, board.result(), total_reward  # Return total_reward\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def piece_value(piece):\n",
    "    if piece is None:\n",
    "        return 0\n",
    "    elif piece.piece_type == chess.PAWN:\n",
    "        return 0.1\n",
    "    elif piece.piece_type == chess.KNIGHT:\n",
    "        return 0.3\n",
    "    elif piece.piece_type == chess.BISHOP:\n",
    "        return 0.3\n",
    "    elif piece.piece_type == chess.ROOK:\n",
    "        return 0.5\n",
    "    elif piece.piece_type == chess.QUEEN:\n",
    "        return 0.9\n",
    "    elif piece.piece_type == chess.KING:\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pgn_data = load_pgn_data(pgn_file_path)\n",
    "\n",
    "# Pre-train the model\n",
    "model(model, pgn_data)\n",
    "\n",
    "# Load the pretrained model\n",
    "#model = load_model(\"/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/chess_bot/benito/carlsen_stock.h5\")  # Provide the correct file path\n",
    "\n",
    "# Use the pretrained_model for inference or further training\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "log_dir = \"logs/\"\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "num_games = 1\n",
    "total_rewards = []\n",
    "results = {\"white_wins\": 0, \"black_wins\": 0, \"draws\": 0}\n",
    "outcomes = []\n",
    "\n",
    "\n",
    "reward_trend = []\n",
    "\n",
    "for episode in tqdm(range(num_games), desc=\"Training\"):\n",
    "    exploration_prob *= 0.99  # Decay exploration probability\n",
    "    print(\"Game:\", episode)\n",
    "    game_states, result, total_reward = play_game()\n",
    "\n",
    "    # Append the total reward to the reward trend list\n",
    "    reward_trend.append(total_reward)\n",
    "\n",
    "    # Update results based on the game outcome\n",
    "    if result == \"1-0\":\n",
    "        results[\"white_wins\"] += 1\n",
    "        outcomes.append(1)\n",
    "    elif result == \"0-1\":\n",
    "        results[\"black_wins\"] += 1\n",
    "        outcomes.append(0)\n",
    "    elif result == \"1/2-1/2\":\n",
    "        results[\"draws\"] += 1\n",
    "        outcomes.append(0.5)  # Fix here: Append 0.5 for draws\n",
    "\n",
    "    total_rewards.append(total_reward)\n",
    "    \n",
    "    # Display the total reward for each game\n",
    "    print(\"Total Reward for Game {}: {}\".format(episode, total_reward))\n",
    "    print(\"Game Outcome:\", result)\n",
    "    #game length\n",
    "    print(\"Game Length:\", len(game_states))\n",
    "\n",
    "# Display statistics\n",
    "average_reward = sum(total_rewards) / num_games\n",
    "print(\"Average Total Reward:\", average_reward)\n",
    "\n",
    "# Extract the FEN of the final position\n",
    "final_position_fen = game_states[-1].fen()\n",
    "print(\"Final Position FEN:\", final_position_fen)\n",
    "\n",
    "# Display the last game\n",
    "for state in game_states:\n",
    "    display_chess_board(state)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nResults Summary:\")\n",
    "print(\"White Wins:\", results[\"white_wins\"])\n",
    "print(\"Black Wins:\", results[\"black_wins\"])\n",
    "print(\"Draws:\", results[\"draws\"])\n",
    "\n",
    "# Plot trend lines\n",
    "plt.plot(outcomes, label=\"Game Outcomes\")\n",
    "plt.xlabel(\"Games\")\n",
    "plt.ylabel(\"Outcome (1 for White Win, 0 for Draw, 0.5 for Loss)\")\n",
    "plt.legend()\n",
    "plt.title(\"Game Outcomes Trend\")\n",
    "\n",
    "# Plot rewards\n",
    "plt.figure()\n",
    "plt.plot(reward_trend, label=\"Rewards\")\n",
    "plt.xlabel(\"Games\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.legend()\n",
    "plt.title(\"Reward Trend\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
