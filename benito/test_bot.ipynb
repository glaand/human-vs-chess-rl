{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stockfish import Stockfish\n",
    "import chess\n",
    "\n",
    "stockfish_path = \"/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/engine/stockfish\"\n",
    "stockfish = Stockfish(path=stockfish_path)\n",
    "\n",
    "def print_board(board):\n",
    "    print(board)\n",
    "\n",
    "def play_game():\n",
    "    board = chess.Board()\n",
    "\n",
    "    print(\"Chess game against Stockfish\\n\")\n",
    "\n",
    "    while not board.is_game_over():\n",
    "        print_board(board)\n",
    "\n",
    "        # Player's move\n",
    "        player_move = input(\"Your move (in algebraic notation): \")\n",
    "        if chess.Move.from_uci(player_move) in board.legal_moves:\n",
    "            board.push_uci(player_move)\n",
    "        else:\n",
    "            print(\"Invalid move. Try again.\")\n",
    "            continue\n",
    "\n",
    "        if board.is_game_over():\n",
    "            break\n",
    "\n",
    "        # Stockfish's move\n",
    "        stockfish.set_fen_position(board.fen())\n",
    "        stockfish_move = stockfish.get_best_move()\n",
    "        print(\"Stockfish's move:\", stockfish_move)\n",
    "        board.push_uci(stockfish_move)\n",
    "\n",
    "    print(\"\\nGame Over\")\n",
    "    print(\"Result:\", board.result())\n",
    "\n",
    "# Play the game\n",
    "#play_game()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "from stockfish import Stockfish\n",
    "import chess\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from tensorflow.compat.v1.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from IPython.display import display, HTML\n",
    "import chess.svg\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Model\n",
    "import chess.pgn\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training on PGN data...\n"
     ]
    }
   ],
   "source": [
    "# Path to Stockfish engine\n",
    "stockfish_path = \"/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/engine/stockfish\"\n",
    "pgn_file_path = '/Users/benitorusconi/Downloads/Stockfish_10_64-bit.bare.[650].pgn'\n",
    "stockfish = Stockfish(path=stockfish_path)\n",
    "\n",
    "#stockfish.set_skill_level(1)\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "discount_factor = 0.85\n",
    "exploration_prob = 0.2\n",
    "\n",
    "# Neural Network Architecture\n",
    "state_space_size = (8, 8, 12)  # 8x8 board with 12 channels (one for each piece type and color)\n",
    "action_space_size = 4096\n",
    "\n",
    "# Initialize a deque for experience replay\n",
    "experience_replay_buffer = deque(maxlen=50000)\n",
    "\n",
    "# Neural Network Model alpha zero\n",
    "\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=state_space_size)\n",
    "\n",
    "# Contracting path\n",
    "# Convolutional layers\n",
    "conv1 = Conv2D(32, (3, 3), activation='selu', padding='same')(input_layer)\n",
    "conv2 = Conv2D(64, (3, 3), activation='selu', padding='same')(conv1)\n",
    "flatten_layer = Flatten()(conv2)\n",
    "dense1 = Dense(64, activation='selu')(flatten_layer)\n",
    "dense2 = Dense(32, activation='selu')(dense1)\n",
    "output_layer = Dense(action_space_size, activation='softmax')(dense2)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def board_to_input_array(board):\n",
    "    board_array = np.zeros((8, 8, 12), dtype=np.uint8)\n",
    "    piece_mapping = {'r': 0, 'n': 1, 'b': 2, 'q': 3, 'k': 4, 'p': 5, 'R': 6, 'N': 7, 'B': 8, 'Q': 9, 'K': 10, 'P': 11}\n",
    "    #normalize piece values\n",
    "\n",
    "    \n",
    "    for square, piece in board.piece_map().items():\n",
    "        piece_type = piece_mapping[piece.symbol()]\n",
    "        color = int(piece.color)\n",
    "        board_array[square // 8, square % 8, piece_type] = color + 1  # Use 0 for empty squares\n",
    "\n",
    "    return board_array\n",
    "\n",
    "# Function to load PGN games and convert them into training data\n",
    "def load_pgn_data(pgn_file_path):\n",
    "    pgn_data = []\n",
    "    with open(pgn_file_path) as pgn:\n",
    "        while True:\n",
    "            game = chess.pgn.read_game(pgn)\n",
    "            if game is None:\n",
    "                break\n",
    "            board = game.board()\n",
    "            for move in game.mainline_moves():\n",
    "                input_array = board_to_input_array(board)\n",
    "                output_array = move_to_output_array(move, board.legal_moves)\n",
    "                pgn_data.append((input_array, output_array))\n",
    "                board.push(move)\n",
    "    return pgn_data\n",
    "\n",
    "# Function to convert a move into an output array\n",
    "def move_to_output_array(move, legal_moves):\n",
    "    output_array = np.zeros(action_space_size)\n",
    "    move_index = list(legal_moves).index(move)\n",
    "    output_array[move_index] = 1\n",
    "    return output_array\n",
    "\n",
    "# Pre-training the model on PGN games\n",
    "def pretrain_model(model, pgn_data):\n",
    "    print(\"Pre-training on PGN data...\")\n",
    "    for input_array, output_array in pgn_data:\n",
    "        # Reshape input for the model, if necessary\n",
    "        input_array = input_array.reshape((1,) + input_array.shape)\n",
    "        output_array = output_array.reshape((1,) + output_array.shape)\n",
    "        model.train_on_batch(input_array, output_array)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def state_to_index(board):\n",
    "    board_array = np.array(board_to_input_array(board))\n",
    "    return hash(board_array.tostring()) % state_space_size[0]\n",
    "\n",
    "\n",
    "\n",
    "def choose_action(board):\n",
    "    if np.random.rand() < exploration_prob:\n",
    "        return np.random.choice(list(board.legal_moves))\n",
    "    else:\n",
    "        state_index = state_to_index(board)\n",
    "        legal_moves_list = list(board.legal_moves)\n",
    "        if not legal_moves_list:\n",
    "            return chess.Move.null()\n",
    "        q_values = model.predict(np.array([board_to_input_array(board)]))[0]\n",
    "        best_move_index = np.argmax(q_values)\n",
    "        best_move_uci = legal_moves_list[min(best_move_index, len(legal_moves_list)-1)].uci()\n",
    "        return chess.Move.from_uci(best_move_uci)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def piece_coordination_reward(board, current_move):\n",
    "    # Evaluate piece coordination as the average number of legal moves for all pieces before and after the move\n",
    "    total_moves_before = 0\n",
    "    total_moves_after = 0\n",
    "    total_pieces = 0\n",
    "\n",
    "    for square, piece in board.piece_map().items():\n",
    "        if piece.color == board.turn:\n",
    "            legal_moves_before = list(board.legal_moves)\n",
    "            total_moves_before += len(legal_moves_before)\n",
    "\n",
    "            # Make a hypothetical move and evaluate legal moves after the move\n",
    "            board_copy = board.copy()\n",
    "            board_copy.push(current_move)\n",
    "            legal_moves_after = list(board_copy.legal_moves)\n",
    "            total_moves_after += len(legal_moves_after)\n",
    "\n",
    "            total_pieces += 1\n",
    "\n",
    "    # Calculate the change in total moves\n",
    "    moves_change = total_moves_after - total_moves_before\n",
    "\n",
    "    # Normalize the reward to be between -1 and 1\n",
    "    max_moves = max(total_moves_before / total_pieces, 1)\n",
    "    normalized_reward = (total_moves_before / total_pieces - 1) / max_moves\n",
    "\n",
    "    # Adjust the reward based on the change in total moves\n",
    "    reward_adjustment = 0.1\n",
    "    normalized_reward += reward_adjustment * moves_change / max_moves\n",
    "\n",
    "    # Scale the reward between -1 and 1\n",
    "    normalized_reward = max(min(normalized_reward, 1), -1)\n",
    "\n",
    "    #print(\"Piece coordination reward:\", normalized_reward)\n",
    "    #print(\"Total moves before:\", total_moves_before)\n",
    "    #print(\"Total moves after:\", total_moves_after)\n",
    "\n",
    "    return normalized_reward\n",
    "\n",
    "def normalize_input(board):\n",
    "    board_array = np.array(board_to_input_array(board), dtype=np.float16)\n",
    "    board_array /= 12.0  # Assuming the maximum piece type value is 12\n",
    "    return board_array\n",
    "\n",
    "\n",
    "\n",
    "def update_q_table(state, action, reward, next_state):\n",
    "    state_index = state_to_index(state)\n",
    "    next_state_index = state_to_index(next_state)\n",
    "    action_index = list(state.legal_moves).index(action)\n",
    "    \n",
    "\n",
    "\n",
    "    # Combine the rewards with weights (you can adjust the weights as needed)\n",
    "    #total_reward = reward + 0.01 * piece_coordination_reward_value\n",
    "    total_reward = reward\n",
    "\n",
    "    # Store the experience in the replay buffer\n",
    "    experience_replay_buffer.append((state_index, action_index, total_reward, next_state_index))\n",
    "\n",
    "    # Sample a batch from the replay buffer for training\n",
    "    batch_size = min(len(experience_replay_buffer), 8)\n",
    "    if batch_size > 0:\n",
    "        batch = np.array(random.sample(experience_replay_buffer, batch_size))\n",
    "        states = np.array([board_to_input_array(chess.Board(fen=chess.STARTING_FEN)) for _ in batch[:, 0]])\n",
    "        next_states = np.array([board_to_input_array(chess.Board(fen=chess.STARTING_FEN)) for _ in batch[:, 3]])\n",
    "        q_values = model.predict(states)\n",
    "        next_q_values = model.predict(next_states)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            action_idx = int(batch[i, 1])  # Cast to integer\n",
    "            q_values[i, action_idx] += learning_rate * (\n",
    "                batch[i, 2] + discount_factor * np.max(next_q_values[i]) - q_values[i, action_idx]\n",
    "            )\n",
    "        \n",
    "        # Train the model on the batch\n",
    "        model.train_on_batch(states, q_values)\n",
    "\n",
    "\n",
    "def display_chess_board(board):\n",
    "    return display(HTML(chess.svg.board(board=board, size=400)))\n",
    "\n",
    "def play_game():\n",
    "    \n",
    "    \n",
    "    custom_fen = \"4k3/8/8/8/8/8/8/2QRK3 w - - 0 1\"  # Replace with your custom FEN string\n",
    "    board = chess.Board(fen=custom_fen)\n",
    "\n",
    "    \n",
    "    game_states = []\n",
    "    total_reward = 0  # Initialize total_reward\n",
    "\n",
    "    while not board.is_game_over():\n",
    "        state = board.copy()\n",
    "        game_states.append(state.copy())\n",
    "\n",
    "        rl_move = choose_action(board)\n",
    "        if rl_move in board.legal_moves:\n",
    "            board.push(rl_move)\n",
    "        else:\n",
    "            print(\"Invalid move. Try again.\")\n",
    "            continue\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        if board.is_game_over():\n",
    "            break\n",
    "\n",
    "        stockfish.set_fen_position(board.fen())\n",
    "        stockfish_move_uci = stockfish.get_best_move()\n",
    "        stockfish_move = chess.Move.from_uci(stockfish_move_uci)\n",
    "        next_state = board.copy()\n",
    "        board.push(stockfish_move)\n",
    "\n",
    "        if next_state.is_check():\n",
    "            reward = 0.1\n",
    "\n",
    "        if board.result() == \"1-0\":\n",
    "            reward += 100  # Win\n",
    "        elif board.result() == \"0-1\":\n",
    "            move_number = len(game_states)\n",
    "            reward -=100 # Loss \n",
    "        elif board.result() == \"1/2-1/2\":\n",
    "            reward -= 100  # Draw\n",
    "\n",
    "        # Capture rewards based on piece values\n",
    "        if board.is_capture(rl_move):\n",
    "            captured_piece_value = piece_value(board.piece_at(rl_move.to_square))\n",
    "            reward += captured_piece_value\n",
    "\n",
    "        if board.is_capture(stockfish_move):\n",
    "            captured_piece_value = piece_value(board.piece_at(stockfish_move.to_square))\n",
    "            reward -= captured_piece_value\n",
    "\n",
    "        # Calculate the change in the number of legal moves\n",
    "        legal_moves_before = len(list(board.legal_moves))\n",
    "        legal_moves_after = len(list(next_state.legal_moves))\n",
    "        moves_change = legal_moves_after - legal_moves_before\n",
    "\n",
    "        # Give a small reward for more possible moves in the future\n",
    "        future_moves_reward = 0.05\n",
    "        reward += future_moves_reward\n",
    "        \n",
    "        if rl_move.uci() == stockfish_move.uci():\n",
    "            move_match_reward = 0.25\n",
    "            reward += move_match_reward\n",
    "\n",
    "\n",
    "        update_q_table(state, rl_move, reward, next_state)\n",
    "\n",
    "        # Accumulate the reward\n",
    "        total_reward += reward\n",
    "\n",
    "\n",
    "\n",
    "    game_states.append(board.copy())\n",
    "    return game_states, board.result(), total_reward  # Return total_reward\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def piece_value(piece):\n",
    "    if piece is None:\n",
    "        return 0\n",
    "    elif piece.piece_type == chess.PAWN:\n",
    "        return 0.1\n",
    "    elif piece.piece_type == chess.KNIGHT:\n",
    "        return 0.3\n",
    "    elif piece.piece_type == chess.BISHOP:\n",
    "        return 0.3\n",
    "    elif piece.piece_type == chess.ROOK:\n",
    "        return 0.5\n",
    "    elif piece.piece_type == chess.QUEEN:\n",
    "        return 0.9\n",
    "    elif piece.piece_type == chess.KING:\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pgn_file_path = '/Users/benitorusconi/Downloads/Stockfish_10_64-bit.bare.[650].pgn'\n",
    "pgn_data = load_pgn_data(pgn_file_path)\n",
    "\n",
    "# Pre-train the model\n",
    "pretrain_model(model, pgn_data)\n",
    "\n",
    "\n",
    "\n",
    "log_dir = \"logs/\"\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_games = 200\n",
    "total_rewards = []\n",
    "results = {\"white_wins\": 0, \"black_wins\": 0, \"draws\": 0}\n",
    "outcomes = []\n",
    "\n",
    "\n",
    "reward_trend = []\n",
    "\n",
    "for episode in tqdm(range(num_games), desc=\"Training\"):\n",
    "    print(\"Game:\", episode)\n",
    "    game_states, result, total_reward = play_game()\n",
    "\n",
    "    # Append the total reward to the reward trend list\n",
    "    reward_trend.append(total_reward)\n",
    "\n",
    "    # Update results based on the game outcome\n",
    "    if result == \"1-0\":\n",
    "        results[\"white_wins\"] += 1\n",
    "        outcomes.append(1)\n",
    "    elif result == \"0-1\":\n",
    "        results[\"black_wins\"] += 1\n",
    "        outcomes.append(0)\n",
    "    elif result == \"1/2-1/2\":\n",
    "        results[\"draws\"] += 1\n",
    "        outcomes.append(0.5)  # Fix here: Append 0.5 for draws\n",
    "\n",
    "    total_rewards.append(total_reward)\n",
    "    \n",
    "    # Display the total reward for each game\n",
    "    print(\"Total Reward for Game {}: {}\".format(episode, total_reward))\n",
    "    print(\"Game Outcome:\", result)\n",
    "    #game length\n",
    "    print(\"Game Length:\", len(game_states))\n",
    "\n",
    "# Display statistics\n",
    "average_reward = sum(total_rewards) / num_games\n",
    "print(\"Average Total Reward:\", average_reward)\n",
    "\n",
    "# Extract the FEN of the final position\n",
    "final_position_fen = game_states[-1].fen()\n",
    "print(\"Final Position FEN:\", final_position_fen)\n",
    "\n",
    "# Display the last game\n",
    "for state in game_states:\n",
    "    display_chess_board(state)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nResults Summary:\")\n",
    "print(\"White Wins:\", results[\"white_wins\"])\n",
    "print(\"Black Wins:\", results[\"black_wins\"])\n",
    "print(\"Draws:\", results[\"draws\"])\n",
    "\n",
    "# Plot trend lines\n",
    "plt.plot(outcomes, label=\"Game Outcomes\")\n",
    "plt.xlabel(\"Games\")\n",
    "plt.ylabel(\"Outcome (1 for White Win, 0 for Draw, 0.5 for Loss)\")\n",
    "plt.legend()\n",
    "plt.title(\"Game Outcomes Trend\")\n",
    "\n",
    "# Plot rewards\n",
    "plt.figure()\n",
    "plt.plot(reward_trend, label=\"Rewards\")\n",
    "plt.xlabel(\"Games\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.legend()\n",
    "plt.title(\"Reward Trend\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
