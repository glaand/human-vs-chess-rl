{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stockfish import Stockfish\n",
    "import chess\n",
    "\n",
    "stockfish_path = \"/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/engine/stockfish\"\n",
    "stockfish = Stockfish(path=stockfish_path)\n",
    "\n",
    "def print_board(board):\n",
    "    print(board)\n",
    "\n",
    "def play_game():\n",
    "    board = chess.Board()\n",
    "\n",
    "    print(\"Chess game against Stockfish\\n\")\n",
    "\n",
    "    while not board.is_game_over():\n",
    "        print_board(board)\n",
    "\n",
    "        # Player's move\n",
    "        player_move = input(\"Your move (in algebraic notation): \")\n",
    "        if chess.Move.from_uci(player_move) in board.legal_moves:\n",
    "            board.push_uci(player_move)\n",
    "        else:\n",
    "            print(\"Invalid move. Try again.\")\n",
    "            continue\n",
    "\n",
    "        if board.is_game_over():\n",
    "            break\n",
    "\n",
    "        # Stockfish's move\n",
    "        stockfish.set_fen_position(board.fen())\n",
    "        stockfish_move = stockfish.get_best_move()\n",
    "        print(\"Stockfish's move:\", stockfish_move)\n",
    "        board.push_uci(stockfish_move)\n",
    "\n",
    "    print(\"\\nGame Over\")\n",
    "    print(\"Result:\", board.result())\n",
    "\n",
    "# Play the game\n",
    "#play_game()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game: 0\n",
      "Game: 1\n",
      "Game: 2\n",
      "Game: 3\n",
      "Game: 4\n",
      "Game: 5\n",
      "Game: 6\n",
      "Game: 7\n",
      "Game: 8\n",
      "Game: 9\n",
      "Game: 10\n",
      "Game: 11\n",
      "Game: 12\n",
      "Game: 13\n",
      "Game: 14\n",
      "Game: 15\n",
      "Game: 16\n",
      "Game: 17\n",
      "Game: 18\n",
      "Game: 19\n",
      "Game: 20\n",
      "Game: 21\n",
      "Game: 22\n",
      "Game: 23\n",
      "Game: 24\n",
      "Game: 25\n",
      "Game: 26\n",
      "Game: 27\n",
      "Game: 28\n",
      "Game: 29\n",
      "Game: 30\n",
      "Game: 31\n",
      "Game: 32\n",
      "Game: 33\n",
      "Game: 34\n",
      "Game: 35\n",
      "Game: 36\n",
      "Game: 37\n",
      "Game: 38\n",
      "Game: 39\n",
      "Game: 40\n",
      "Game: 41\n",
      "Game: 42\n",
      "Game: 43\n",
      "Game: 44\n",
      "Game: 45\n",
      "Game: 46\n",
      "Game: 47\n",
      "Game: 48\n",
      "Game: 49\n",
      "Game: 50\n",
      "Game: 51\n",
      "Game: 52\n",
      "Game: 53\n",
      "Game: 54\n",
      "Game: 55\n",
      "Game: 56\n",
      "Game: 57\n",
      "Game: 58\n",
      "Game: 59\n",
      "Game: 60\n",
      "Game: 61\n",
      "Game: 62\n",
      "Game: 63\n",
      "Game: 64\n",
      "Game: 65\n",
      "Game: 66\n"
     ]
    }
   ],
   "source": [
    "from stockfish import Stockfish\n",
    "import chess\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.compat.v1.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from IPython.display import display, HTML\n",
    "import chess.svg\n",
    "\n",
    "stockfish_path = \"/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/engine/stockfish\"\n",
    "stockfish = Stockfish(path=stockfish_path)\n",
    "\n",
    "learning_rate = 0.001\n",
    "discount_factor = 0.9\n",
    "exploration_prob = 0.4\n",
    "\n",
    "state_space_size = 64\n",
    "action_space_size = 1000\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, input_shape=(state_space_size,), activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(action_space_size, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "\n",
    "def state_to_index(board):\n",
    "    return hash(board.fen()) % state_space_size\n",
    "\n",
    "def choose_action(board):\n",
    "    if np.random.rand() < exploration_prob:\n",
    "        return np.random.choice(list(board.legal_moves))\n",
    "    else:\n",
    "        state_index = state_to_index(board)\n",
    "        legal_moves_list = list(board.legal_moves)\n",
    "        if not legal_moves_list:\n",
    "            return chess.Move.null()\n",
    "        best_move_index = np.argmax(q_table[state_index])\n",
    "        best_move_uci = legal_moves_list[min(best_move_index, len(legal_moves_list)-1)].uci()\n",
    "        return chess.Move.from_uci(best_move_uci)\n",
    "\n",
    "def update_q_table(state, action, reward, next_state):\n",
    "    state_index = state_to_index(state)\n",
    "    next_state_index = state_to_index(next_state)\n",
    "    action_index = list(state.legal_moves).index(action)\n",
    "    best_next_action = np.argmax(q_table[next_state_index])\n",
    "    q_table[state_index, action_index] += learning_rate * (\n",
    "        reward + discount_factor * q_table[next_state_index, best_next_action] - q_table[state_index, action_index]\n",
    "    )\n",
    "\n",
    "def display_chess_board(board):\n",
    "    return display(HTML(chess.svg.board(board=board, size=400)))\n",
    "\n",
    "def play_game():\n",
    "    board = chess.Board()\n",
    "    game_states = []\n",
    "\n",
    "    while not board.is_game_over():\n",
    "        state = board.copy()\n",
    "        game_states.append(state.copy())\n",
    "\n",
    "        rl_move = choose_action(board)\n",
    "        if rl_move in board.legal_moves:\n",
    "            board.push(rl_move)\n",
    "        else:\n",
    "            print(\"Invalid move. Try again.\")\n",
    "            continue\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        if board.is_game_over():\n",
    "            break\n",
    "\n",
    "        stockfish.set_fen_position(board.fen())\n",
    "        stockfish_move_uci = stockfish.get_best_move()\n",
    "        stockfish_move = chess.Move.from_uci(stockfish_move_uci)\n",
    "        next_state = board.copy()\n",
    "        board.push(stockfish_move)\n",
    "\n",
    "        if next_state.is_check():\n",
    "            reward = 0.5\n",
    "\n",
    "        if board.result() == \"1-0\":\n",
    "            reward = 1000 # Win\n",
    "        elif board.result() == \"0-1\":\n",
    "            reward = -1000  # Loss\n",
    "        elif board.result() == \"1/2-1/2\":\n",
    "            reward = 100  # Draw\n",
    "\n",
    "        # Capture rewards based on piece values\n",
    "        if board.is_capture(rl_move):\n",
    "            captured_piece_value = piece_value(board.piece_at(rl_move.to_square))\n",
    "            reward += captured_piece_value\n",
    "\n",
    "        if board.is_capture(stockfish_move):\n",
    "            captured_piece_value = piece_value(board.piece_at(stockfish_move.to_square))\n",
    "            reward -= captured_piece_value *1.01\n",
    "\n",
    "        update_q_table(state, rl_move, reward, next_state)\n",
    "\n",
    "    game_states.append(board.copy())\n",
    "    return game_states, board.result()\n",
    "\n",
    "def piece_value(piece):\n",
    "    # Assign values to pieces\n",
    "    if piece is None:\n",
    "        return 0\n",
    "    elif piece.piece_type == chess.PAWN:\n",
    "        return 1\n",
    "    elif piece.piece_type == chess.KNIGHT:\n",
    "        return 3\n",
    "    elif piece.piece_type == chess.BISHOP:\n",
    "        return 3\n",
    "    elif piece.piece_type == chess.ROOK:\n",
    "        return 5\n",
    "    elif piece.piece_type == chess.QUEEN:\n",
    "        return 9\n",
    "    elif piece.piece_type == chess.KING:\n",
    "        return 10\n",
    "\n",
    "\n",
    "log_dir = \"logs/\"\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "num_games = 100\n",
    "for episode in range(num_games):\n",
    "    print(\"Game:\", episode)\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    game_states, result = play_game()\n",
    "\n",
    "    with tf.summary.create_file_writer(log_dir).as_default():\n",
    "        tf.summary.scalar('Total Reward', total_reward, step=episode)\n",
    "        tf.summary.scalar('Steps', steps, step=episode)\n",
    "        tf.summary.flush()\n",
    "\n",
    "        for layer in model.layers:\n",
    "            for weight in layer.weights:\n",
    "                tf.summary.histogram(weight.name, weight, step=episode)\n",
    "\n",
    "    # Calculate and print the sum of absolute deviations for each game\n",
    "    sum_deviation = 0.0\n",
    "\n",
    "    if episode == num_games - 1:\n",
    "        # Display the last game\n",
    "        for state in game_states:\n",
    "            display_chess_board(state)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
