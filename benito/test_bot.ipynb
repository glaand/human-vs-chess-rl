{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stockfish import Stockfish\n",
    "import chess\n",
    "\n",
    "stockfish_path = \"/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/engine/stockfish\"\n",
    "stockfish = Stockfish(path=stockfish_path)\n",
    "\n",
    "def print_board(board):\n",
    "    print(board)\n",
    "\n",
    "def play_game():\n",
    "    board = chess.Board()\n",
    "\n",
    "    print(\"Chess game against Stockfish\\n\")\n",
    "\n",
    "    while not board.is_game_over():\n",
    "        print_board(board)\n",
    "\n",
    "        # Player's move\n",
    "        player_move = input(\"Your move (in algebraic notation): \")\n",
    "        if chess.Move.from_uci(player_move) in board.legal_moves:\n",
    "            board.push_uci(player_move)\n",
    "        else:\n",
    "            print(\"Invalid move. Try again.\")\n",
    "            continue\n",
    "\n",
    "        if board.is_game_over():\n",
    "            break\n",
    "\n",
    "        # Stockfish's move\n",
    "        stockfish.set_fen_position(board.fen())\n",
    "        stockfish_move = stockfish.get_best_move()\n",
    "        print(\"Stockfish's move:\", stockfish_move)\n",
    "        board.push_uci(stockfish_move)\n",
    "\n",
    "    print(\"\\nGame Over\")\n",
    "    print(\"Result:\", board.result())\n",
    "\n",
    "# Play the game\n",
    "#play_game()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_22 (InputLayer)       [(None, 8, 8, 12)]        0         \n",
      "                                                                 \n",
      " conv2d_42 (Conv2D)          (None, 8, 8, 32)          3488      \n",
      "                                                                 \n",
      " conv2d_43 (Conv2D)          (None, 8, 8, 64)          18496     \n",
      "                                                                 \n",
      " flatten_21 (Flatten)        (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 64)                262208    \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 4096)              135168    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 421440 (1.61 MB)\n",
      "Trainable params: 421440 (1.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnProcess-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/anaconda3/envs/RL/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/anaconda3/envs/RL/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/anaconda3/envs/RL/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/homebrew/anaconda3/envs/RL/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'play_game' on <module '__main__' (built-in)>\n",
      "Process SpawnProcess-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/anaconda3/envs/RL/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/anaconda3/envs/RL/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/anaconda3/envs/RL/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/homebrew/anaconda3/envs/RL/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'play_game' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A child process terminated abruptly, the process pool is not usable anymore",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/chess_bot/benito/test_bot.ipynb Cell 3\u001b[0m line \u001b[0;36m<cell line: 327>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/benito/test_bot.ipynb#W6sZmlsZQ%3D%3D?line=324'>325</a>\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/benito/test_bot.ipynb#W6sZmlsZQ%3D%3D?line=326'>327</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/benito/test_bot.ipynb#W6sZmlsZQ%3D%3D?line=327'>328</a>\u001b[0m     play_multiple_games(num_games\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32m/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/chess_bot/benito/test_bot.ipynb Cell 3\u001b[0m line \u001b[0;36mplay_multiple_games\u001b[0;34m(num_games)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/benito/test_bot.ipynb#W6sZmlsZQ%3D%3D?line=282'>283</a>\u001b[0m total_rewards \u001b[39m=\u001b[39m []\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/benito/test_bot.ipynb#W6sZmlsZQ%3D%3D?line=284'>285</a>\u001b[0m \u001b[39mwith\u001b[39;00m ProcessPoolExecutor(max_workers\u001b[39m=\u001b[39mmultiprocessing\u001b[39m.\u001b[39mcpu_count()) \u001b[39mas\u001b[39;00m executor:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/benito/test_bot.ipynb#W6sZmlsZQ%3D%3D?line=285'>286</a>\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(tqdm(executor\u001b[39m.\u001b[39;49mmap(play_game, \u001b[39mrange\u001b[39;49m(num_games)), total\u001b[39m=\u001b[39mnum_games, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/benito/test_bot.ipynb#W6sZmlsZQ%3D%3D?line=287'>288</a>\u001b[0m \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m results:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/benito/test_bot.ipynb#W6sZmlsZQ%3D%3D?line=288'>289</a>\u001b[0m     game_states, _, total_reward \u001b[39m=\u001b[39m result\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/RL/lib/python3.10/concurrent/futures/process.py:766\u001b[0m, in \u001b[0;36mProcessPoolExecutor.map\u001b[0;34m(self, fn, timeout, chunksize, *iterables)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    764\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mchunksize must be >= 1.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 766\u001b[0m results \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mmap(partial(_process_chunk, fn),\n\u001b[1;32m    767\u001b[0m                       _get_chunks(\u001b[39m*\u001b[39;49miterables, chunksize\u001b[39m=\u001b[39;49mchunksize),\n\u001b[1;32m    768\u001b[0m                       timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    769\u001b[0m \u001b[39mreturn\u001b[39;00m _chain_from_iterable_of_lists(results)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/RL/lib/python3.10/concurrent/futures/_base.py:610\u001b[0m, in \u001b[0;36mExecutor.map\u001b[0;34m(self, fn, timeout, chunksize, *iterables)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     end_time \u001b[39m=\u001b[39m timeout \u001b[39m+\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 610\u001b[0m fs \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubmit(fn, \u001b[39m*\u001b[39margs) \u001b[39mfor\u001b[39;00m args \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39miterables)]\n\u001b[1;32m    612\u001b[0m \u001b[39m# Yield must be hidden in closure so that the futures are submitted\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[39m# before the first iterator value is required.\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mresult_iterator\u001b[39m():\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/RL/lib/python3.10/concurrent/futures/_base.py:610\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     end_time \u001b[39m=\u001b[39m timeout \u001b[39m+\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 610\u001b[0m fs \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubmit(fn, \u001b[39m*\u001b[39;49margs) \u001b[39mfor\u001b[39;00m args \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39miterables)]\n\u001b[1;32m    612\u001b[0m \u001b[39m# Yield must be hidden in closure so that the futures are submitted\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[39m# before the first iterator value is required.\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mresult_iterator\u001b[39m():\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/RL/lib/python3.10/concurrent/futures/process.py:720\u001b[0m, in \u001b[0;36mProcessPoolExecutor.submit\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown_lock:\n\u001b[1;32m    719\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_broken:\n\u001b[0;32m--> 720\u001b[0m         \u001b[39mraise\u001b[39;00m BrokenProcessPool(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_broken)\n\u001b[1;32m    721\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown_thread:\n\u001b[1;32m    722\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mcannot schedule new futures after shutdown\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A child process terminated abruptly, the process pool is not usable anymore"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "from stockfish import Stockfish\n",
    "import chess\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from tensorflow.compat.v1.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from IPython.display import display, HTML\n",
    "import chess.svg\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "\n",
    "# Path to Stockfish engine\n",
    "stockfish_path = \"/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/engine/stockfish\"\n",
    "stockfish = Stockfish(path=stockfish_path)\n",
    "\n",
    "#stockfish.set_skill_level(1)\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "discount_factor = 0.85\n",
    "exploration_prob = 0.2\n",
    "\n",
    "# Neural Network Architecture\n",
    "state_space_size = (8, 8, 12)  # 8x8 board with 12 channels (one for each piece type and color)\n",
    "action_space_size = 4096\n",
    "\n",
    "# Initialize a deque for experience replay\n",
    "experience_replay_buffer = deque(maxlen=10000)\n",
    "\n",
    "# Neural Network Model alpha zero\n",
    "\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=state_space_size)\n",
    "\n",
    "# Contracting path\n",
    "# Convolutional layers\n",
    "conv1 = Conv2D(32, (3, 3), activation='selu', padding='same')(input_layer)\n",
    "conv2 = Conv2D(64, (3, 3), activation='selu', padding='same')(conv1)\n",
    "\n",
    "\n",
    "# Flatten layer\n",
    "flatten_layer = Flatten()(conv2)\n",
    "\n",
    "# Dense layers\n",
    "dense1 = Dense(64, activation='selu')(flatten_layer)\n",
    "dense2 = Dense(32, activation='selu')(dense1)\n",
    "\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(action_space_size, activation='softmax')(dense2)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "def state_to_index(board):\n",
    "    board_array = np.array(board_to_input_array(board))\n",
    "    return hash(board_array.tostring()) % state_space_size[0]\n",
    "\n",
    "def board_to_input_array(board):\n",
    "    board_array = np.zeros((8, 8, 12), dtype=np.uint8)\n",
    "    piece_mapping = {'r': 0, 'n': 1, 'b': 2, 'q': 3, 'k': 4, 'p': 5, 'R': 6, 'N': 7, 'B': 8, 'Q': 9, 'K': 10, 'P': 11}\n",
    "    #normalize piece values\n",
    "\n",
    "    \n",
    "    for square, piece in board.piece_map().items():\n",
    "        piece_type = piece_mapping[piece.symbol()]\n",
    "        color = int(piece.color)\n",
    "        board_array[square // 8, square % 8, piece_type] = color + 1  # Use 0 for empty squares\n",
    "\n",
    "    return board_array\n",
    "\n",
    "def choose_action(board):\n",
    "    if np.random.rand() < exploration_prob:\n",
    "        return np.random.choice(list(board.legal_moves))\n",
    "    else:\n",
    "        state_index = state_to_index(board)\n",
    "        legal_moves_list = list(board.legal_moves)\n",
    "        if not legal_moves_list:\n",
    "            return chess.Move.null()\n",
    "        q_values = model.predict(np.array([board_to_input_array(board)]))[0]\n",
    "        best_move_index = np.argmax(q_values)\n",
    "        best_move_uci = legal_moves_list[min(best_move_index, len(legal_moves_list)-1)].uci()\n",
    "        return chess.Move.from_uci(best_move_uci)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def piece_coordination_reward(board, current_move):\n",
    "    # Evaluate piece coordination as the average number of legal moves for all pieces before and after the move\n",
    "    total_moves_before = 0\n",
    "    total_moves_after = 0\n",
    "    total_pieces = 0\n",
    "\n",
    "    for square, piece in board.piece_map().items():\n",
    "        if piece.color == board.turn:\n",
    "            legal_moves_before = list(board.legal_moves)\n",
    "            total_moves_before += len(legal_moves_before)\n",
    "\n",
    "            # Make a hypothetical move and evaluate legal moves after the move\n",
    "            board_copy = board.copy()\n",
    "            board_copy.push(current_move)\n",
    "            legal_moves_after = list(board_copy.legal_moves)\n",
    "            total_moves_after += len(legal_moves_after)\n",
    "\n",
    "            total_pieces += 1\n",
    "\n",
    "    # Calculate the change in total moves\n",
    "    moves_change = total_moves_after - total_moves_before\n",
    "\n",
    "    # Normalize the reward to be between -1 and 1\n",
    "    max_moves = max(total_moves_before / total_pieces, 1)\n",
    "    normalized_reward = (total_moves_before / total_pieces - 1) / max_moves\n",
    "\n",
    "    # Adjust the reward based on the change in total moves\n",
    "    reward_adjustment = 0.1\n",
    "    normalized_reward += reward_adjustment * moves_change / max_moves\n",
    "\n",
    "    # Scale the reward between -1 and 1\n",
    "    normalized_reward = max(min(normalized_reward, 1), -1)\n",
    "\n",
    "    #print(\"Piece coordination reward:\", normalized_reward)\n",
    "    #print(\"Total moves before:\", total_moves_before)\n",
    "    #print(\"Total moves after:\", total_moves_after)\n",
    "\n",
    "    return normalized_reward\n",
    "\n",
    "def normalize_input(board):\n",
    "    board_array = np.array(board_to_input_array(board), dtype=np.float16)\n",
    "    board_array /= 12.0  # Assuming the maximum piece type value is 12\n",
    "    return board_array\n",
    "\n",
    "\n",
    "\n",
    "def update_q_table(state, action, reward, next_state):\n",
    "    state_index = state_to_index(state)\n",
    "    next_state_index = state_to_index(next_state)\n",
    "    action_index = list(state.legal_moves).index(action)\n",
    "    \n",
    "    # Calculate the additional rewards\n",
    "    piece_coordination_reward_value = piece_coordination_reward(state, action)\n",
    "\n",
    "    # Combine the rewards with weights (you can adjust the weights as needed)\n",
    "    #total_reward = reward + 0.01 * piece_coordination_reward_value\n",
    "    total_reward = reward\n",
    "\n",
    "    # Store the experience in the replay buffer\n",
    "    experience_replay_buffer.append((state_index, action_index, total_reward, next_state_index))\n",
    "\n",
    "    # Sample a batch from the replay buffer for training\n",
    "    batch_size = min(len(experience_replay_buffer), 8)\n",
    "    if batch_size > 0:\n",
    "        batch = np.array(random.sample(experience_replay_buffer, batch_size))\n",
    "        states = np.array([board_to_input_array(chess.Board(fen=chess.STARTING_FEN)) for _ in batch[:, 0]])\n",
    "        next_states = np.array([board_to_input_array(chess.Board(fen=chess.STARTING_FEN)) for _ in batch[:, 3]])\n",
    "        q_values = model.predict(states)\n",
    "        next_q_values = model.predict(next_states)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            action_idx = int(batch[i, 1])  # Cast to integer\n",
    "            q_values[i, action_idx] += learning_rate * (\n",
    "                batch[i, 2] + discount_factor * np.max(next_q_values[i]) - q_values[i, action_idx]\n",
    "            )\n",
    "        \n",
    "        # Train the model on the batch\n",
    "        model.train_on_batch(states, q_values)\n",
    "\n",
    "\n",
    "def display_chess_board(board):\n",
    "    return display(HTML(chess.svg.board(board=board, size=400)))\n",
    "\n",
    "def play_game():\n",
    "    \n",
    "    \n",
    "    board = chess.Board()\n",
    "    game_states = []\n",
    "    total_reward = 0  # Initialize total_reward\n",
    "\n",
    "    while not board.is_game_over():\n",
    "        state = board.copy()\n",
    "        game_states.append(state.copy())\n",
    "\n",
    "        rl_move = choose_action(board)\n",
    "        if rl_move in board.legal_moves:\n",
    "            board.push(rl_move)\n",
    "        else:\n",
    "            print(\"Invalid move. Try again.\")\n",
    "            continue\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        if board.is_game_over():\n",
    "            break\n",
    "\n",
    "        stockfish.set_fen_position(board.fen())\n",
    "        stockfish_move_uci = stockfish.get_best_move()\n",
    "        stockfish_move = chess.Move.from_uci(stockfish_move_uci)\n",
    "        next_state = board.copy()\n",
    "        board.push(stockfish_move)\n",
    "\n",
    "        if next_state.is_check():\n",
    "            reward = 0.1\n",
    "\n",
    "        if board.result() == \"1-0\":\n",
    "            reward = 1  # Win\n",
    "        elif board.result() == \"0-1\":\n",
    "            move_number = len(game_states)\n",
    "            reward -=(1+100/move_number)  # Loss \n",
    "        elif board.result() == \"1/2-1/2\":\n",
    "            reward += 0.01  # Draw\n",
    "\n",
    "        # Capture rewards based on piece values\n",
    "        if board.is_capture(rl_move):\n",
    "            captured_piece_value = piece_value(board.piece_at(rl_move.to_square))\n",
    "            reward += captured_piece_value\n",
    "\n",
    "        if board.is_capture(stockfish_move):\n",
    "            captured_piece_value = piece_value(board.piece_at(stockfish_move.to_square))\n",
    "            reward -= captured_piece_value\n",
    "\n",
    "        # Calculate the change in the number of legal moves\n",
    "        legal_moves_before = len(list(board.legal_moves))\n",
    "        legal_moves_after = len(list(next_state.legal_moves))\n",
    "        moves_change = legal_moves_after - legal_moves_before\n",
    "\n",
    "        # Give a small reward for more possible moves in the future\n",
    "        future_moves_reward = 0.05\n",
    "        reward += future_moves_reward\n",
    "        \n",
    "        if rl_move.uci() == stockfish_move.uci():\n",
    "            move_match_reward = 0.25\n",
    "            reward += move_match_reward\n",
    "\n",
    "\n",
    "        update_q_table(state, rl_move, reward, next_state)\n",
    "\n",
    "        # Accumulate the reward\n",
    "        total_reward += reward\n",
    "\n",
    "\n",
    "\n",
    "    game_states.append(board.copy())\n",
    "    return game_states, board.result(), total_reward  # Return total_reward\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def piece_value(piece):\n",
    "    if piece is None:\n",
    "        return 0\n",
    "    elif piece.piece_type == chess.PAWN:\n",
    "        return 0.1\n",
    "    elif piece.piece_type == chess.KNIGHT:\n",
    "        return 0.3\n",
    "    elif piece.piece_type == chess.BISHOP:\n",
    "        return 0.3\n",
    "    elif piece.piece_type == chess.ROOK:\n",
    "        return 0.5\n",
    "    elif piece.piece_type == chess.QUEEN:\n",
    "        return 0.9\n",
    "    elif piece.piece_type == chess.KING:\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "log_dir = \"logs/\"\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_games = 1000\n",
    "total_rewards = []\n",
    "results = {\"white_wins\": 0, \"black_wins\": 0, \"draws\": 0}\n",
    "outcomes = []\n",
    "\n",
    "\n",
    "reward_trend = []\n",
    "\n",
    "for episode in tqdm(range(num_games), desc=\"Training\"):\n",
    "    print(\"Game:\", episode)\n",
    "    game_states, result, total_reward = play_game()\n",
    "\n",
    "    # Append the total reward to the reward trend list\n",
    "    reward_trend.append(total_reward)\n",
    "\n",
    "    # Update results based on the game outcome\n",
    "    if result == \"1-0\":\n",
    "        results[\"white_wins\"] += 1\n",
    "        outcomes.append(1)\n",
    "    elif result == \"0-1\":\n",
    "        results[\"black_wins\"] += 1\n",
    "        outcomes.append(0)\n",
    "    elif result == \"1/2-1/2\":\n",
    "        results[\"draws\"] += 1\n",
    "        outcomes.append(0.5)  # Fix here: Append 0.5 for draws\n",
    "\n",
    "    total_rewards.append(total_reward)\n",
    "    \n",
    "    # Display the total reward for each game\n",
    "    print(\"Total Reward for Game {}: {}\".format(episode, total_reward))\n",
    "    print(\"Game Outcome:\", result)\n",
    "    #game length\n",
    "    print(\"Game Length:\", len(game_states))\n",
    "\n",
    "# Display statistics\n",
    "average_reward = sum(total_rewards) / num_games\n",
    "print(\"Average Total Reward:\", average_reward)\n",
    "\n",
    "# Extract the FEN of the final position\n",
    "final_position_fen = game_states[-1].fen()\n",
    "print(\"Final Position FEN:\", final_position_fen)\n",
    "\n",
    "# Display the last game\n",
    "for state in game_states:\n",
    "    display_chess_board(state)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nResults Summary:\")\n",
    "print(\"White Wins:\", results[\"white_wins\"])\n",
    "print(\"Black Wins:\", results[\"black_wins\"])\n",
    "print(\"Draws:\", results[\"draws\"])\n",
    "\n",
    "# Plot trend lines\n",
    "plt.plot(outcomes, label=\"Game Outcomes\")\n",
    "plt.xlabel(\"Games\")\n",
    "plt.ylabel(\"Outcome (1 for White Win, 0 for Draw, 0.5 for Loss)\")\n",
    "plt.legend()\n",
    "plt.title(\"Game Outcomes Trend\")\n",
    "\n",
    "# Plot rewards\n",
    "plt.figure()\n",
    "plt.plot(reward_trend, label=\"Rewards\")\n",
    "plt.xlabel(\"Games\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.legend()\n",
    "plt.title(\"Reward Trend\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
