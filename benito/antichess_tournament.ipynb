{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import chess\n",
    "import chess.variant\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Input\n",
    "from tensorflow.compat.v1.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from IPython.display import display, HTML\n",
    "import chess.svg\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Chess Variant Antichess\n",
    "\n",
    "def board_to_input_array(board):\n",
    "    board_array = np.zeros((8, 8, 12), dtype=np.uint8)\n",
    "    piece_mapping = {'r': 0, 'n': 1, 'b': 2, 'q': 3, 'k': 4, 'p': 5, 'R': 6, 'N': 7, 'B': 8, 'Q': 9, 'K': 10, 'P': 11}\n",
    "\n",
    "    for square, piece in board.piece_map().items():\n",
    "        piece_type = piece_mapping[piece.symbol()]\n",
    "        color = int(piece.color)\n",
    "        board_array[square // 8, square % 8, piece_type] = color + 1\n",
    "    return board_array\n",
    "\n",
    "def state_to_index(board):\n",
    "    board_array = np.array(board_to_input_array(board))\n",
    "    return hash(board_array.tostring()) % state_space_size[0]\n",
    "\n",
    "def choose_action(board, model):\n",
    "    if np.random.rand() < exploration_prob:\n",
    "        return np.random.choice(list(board.legal_moves))\n",
    "    else:\n",
    "        state_index = state_to_index(board)\n",
    "        legal_moves_list = list(board.legal_moves)\n",
    "        if not legal_moves_list:\n",
    "            return chess.Move.null()\n",
    "        q_values = model.predict(np.array([board_to_input_array(board)]))[0]\n",
    "        best_move_index = np.argmax(q_values)\n",
    "        best_move_uci = legal_moves_list[min(best_move_index, len(legal_moves_list)-1)].uci()\n",
    "        return chess.Move.from_uci(best_move_uci)\n",
    "\n",
    "def move_to_output_array(move, legal_moves):\n",
    "    output_array = np.zeros(action_space_size)\n",
    "    move_index = list(legal_moves).index(move)\n",
    "    output_array[move_index] = 1\n",
    "    return output_array\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "discount_factor = 0.99\n",
    "exploration_prob = 0.2\n",
    "state_space_size = (8, 8, 12)\n",
    "action_space_size = 4096\n",
    "experience_replay_buffer = deque(maxlen=10000)\n",
    "\n",
    "# Neural Network Model alpha zero\n",
    "input_layer = Input(shape=state_space_size)\n",
    "conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)\n",
    "flatten_layer = Flatten()(conv2)\n",
    "dense1 = Dense(64, activation='relu')(flatten_layer)\n",
    "dense2 = Dense(64, activation='relu')(dense1)\n",
    "output_layer = Dense(action_space_size, activation='softmax')(dense2)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.1), loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "\n",
    "def count_pieces_by_color(board, color):\n",
    "    piece_types = [chess.PAWN, chess.KNIGHT, chess.BISHOP, chess.ROOK, chess.QUEEN, chess.KING]\n",
    "    return sum(len(board.pieces(piece_type, color)) for piece_type in piece_types)\n",
    "\n",
    "def normalize_input(board):\n",
    "    board_array = np.array(board_to_input_array(board), dtype=np.float16)\n",
    "    board_array /= 12.0\n",
    "    return board_array\n",
    "\n",
    "def update_q_table(state, action, reward, next_state):\n",
    "    state_index = state_to_index(state)\n",
    "    next_state_index = state_to_index(next_state)\n",
    "    action_index = list(state.legal_moves).index(action)\n",
    "\n",
    "    total_reward = reward\n",
    "    experience_replay_buffer.append((state_index, action_index, total_reward, next_state_index))\n",
    "    batch_size = min(len(experience_replay_buffer), 8)\n",
    "    if batch_size > 0:\n",
    "        batch = np.array(random.sample(experience_replay_buffer, batch_size))\n",
    "        states = np.array([board_to_input_array(chess.Board(fen=chess.STARTING_FEN)) for _ in batch[:, 0]])\n",
    "        next_states = np.array([board_to_input_array(chess.Board(fen=chess.STARTING_FEN)) for _ in batch[:, 3]])\n",
    "        q_values = model.predict(states)\n",
    "        next_q_values = model.predict(next_states)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            action_idx = int(batch[i, 1])\n",
    "            q_values[i, action_idx] += learning_rate * (batch[i, 2] + discount_factor * np.max(next_q_values[i]) - q_values[i, action_idx])\n",
    "        \n",
    "        model.train_on_batch(states, q_values)\n",
    "\n",
    "def calculate_reward(board):\n",
    "    reward = 0\n",
    "    piece_count = len(board.piece_map())\n",
    "    reward -= (32 - piece_count) * 0.1\n",
    "\n",
    "    if board.is_stalemate() or board.is_insufficient_material():\n",
    "        reward -= 5\n",
    "    elif board.is_fivefold_repetition() or board.is_seventyfive_moves():\n",
    "        reward -= 5\n",
    "    return reward\n",
    "\n",
    "def create_new_model():\n",
    "    new_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    new_model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.1), loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "    return new_model\n",
    "\n",
    "def train_model_self_play(num_games, model):\n",
    "    for _ in range(num_games):\n",
    "        play_game(model, model)\n",
    "\n",
    "def play_game(model1, model2):\n",
    "    board = chess.variant.GiveawayBoard()\n",
    "    while not board.is_game_over():\n",
    "        if board.turn == chess.WHITE:\n",
    "            move = choose_action(board, model1)\n",
    "        else:\n",
    "            move = choose_action(board, model2)\n",
    "        board.push(move)\n",
    "    return board.result()\n",
    "\n",
    "def train_new_player(best_player_model, new_player_model, threshold_win_rate=0.55, num_games=200):\n",
    "    new_player_wins = 0\n",
    "    for game in range(num_games):\n",
    "        if random.choice([True, False]):\n",
    "            result = play_game(new_player_model, best_player_model)\n",
    "            if result == \"1-0\":\n",
    "                new_player_wins += 1\n",
    "        else:\n",
    "            result = play_game(best_player_model, new_player_model)\n",
    "            if result == \"0-1\":\n",
    "                new_player_wins += 1\n",
    "\n",
    "        win_rate = new_player_wins / (game + 1)\n",
    "        if win_rate >= threshold_win_rate:\n",
    "            print(f\"New player has achieved a win rate of {win_rate}. It becomes the best player.\")\n",
    "            return new_player_model\n",
    "\n",
    "    print(f\"New player did not achieve the required win rate. Best player remains unchanged.\")\n",
    "    return best_player_model\n",
    "\n",
    "# Load or create initial best player model\n",
    "try:\n",
    "    best_player_model = load_model(\"best_player.h5\")\n",
    "except IOError:\n",
    "    print(\"No initial model found. Training a new model.\")\n",
    "    best_player_model = create_new_model()\n",
    "    train_model_self_play(200, best_player_model)\n",
    "\n",
    "# Main training and updating loop\n",
    "while True:\n",
    "    new_player_model = create_new_model()\n",
    "    best_player_model = train_new_player(best_player_model, new_player_model)\n",
    "    best_player_model.save(\"best_player.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
