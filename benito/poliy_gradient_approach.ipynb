{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game: 0\n",
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'a' and 'p' must have same size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/chess_bot/benito/poliy_gradient_approach.ipynb Cell 1\u001b[0m line \u001b[0;36m<cell line: 190>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/benito/poliy_gradient_approach.ipynb#W2sZmlsZQ%3D%3D?line=190'>191</a>\u001b[0m exploration_prob \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.99\u001b[39m  \u001b[39m# Decay exploration probability\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/benito/poliy_gradient_approach.ipynb#W2sZmlsZQ%3D%3D?line=191'>192</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGame:\u001b[39m\u001b[39m\"\u001b[39m, episode)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/benito/poliy_gradient_approach.ipynb#W2sZmlsZQ%3D%3D?line=192'>193</a>\u001b[0m game_states, result, total_reward \u001b[39m=\u001b[39m play_game()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/benito/poliy_gradient_approach.ipynb#W2sZmlsZQ%3D%3D?line=194'>195</a>\u001b[0m \u001b[39m# Append the total reward to the reward trend list\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/benito/poliy_gradient_approach.ipynb#W2sZmlsZQ%3D%3D?line=195'>196</a>\u001b[0m reward_trend\u001b[39m.\u001b[39mappend(total_reward)\n",
      "\u001b[1;32m/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/chess_bot/benito/poliy_gradient_approach.ipynb Cell 1\u001b[0m line \u001b[0;36mplay_game\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/benito/poliy_gradient_approach.ipynb#W2sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m game_states\u001b[39m.\u001b[39mappend(state\u001b[39m.\u001b[39mcopy())\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/benito/poliy_gradient_approach.ipynb#W2sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m \u001b[39m# Choose an action using the policy network\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/benito/poliy_gradient_approach.ipynb#W2sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m policy_move \u001b[39m=\u001b[39m choose_policy_action(board, policy_model)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/benito/poliy_gradient_approach.ipynb#W2sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m \u001b[39mif\u001b[39;00m policy_move \u001b[39min\u001b[39;00m board\u001b[39m.\u001b[39mlegal_moves:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/benito/poliy_gradient_approach.ipynb#W2sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m     board\u001b[39m.\u001b[39mpush(policy_move)\n",
      "\u001b[1;32m/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement Learning (cds-117)/chess_bot/benito/poliy_gradient_approach.ipynb Cell 1\u001b[0m line \u001b[0;36mchoose_policy_action\u001b[0;34m(board, model)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/benito/poliy_gradient_approach.ipynb#W2sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m action_probs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(state_array)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/benito/poliy_gradient_approach.ipynb#W2sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m legal_moves_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(board\u001b[39m.\u001b[39mlegal_moves)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/benito/poliy_gradient_approach.ipynb#W2sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m action_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice(\u001b[39mrange\u001b[39;49m(\u001b[39mlen\u001b[39;49m(legal_moves_list)), p\u001b[39m=\u001b[39;49maction_probs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/benitorusconi/Documents/CDS/05_HS23/Reinforcement%20Learning%20%28cds-117%29/chess_bot/benito/poliy_gradient_approach.ipynb#W2sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m \u001b[39mreturn\u001b[39;00m legal_moves_list[action_index]\n",
      "File \u001b[0;32mmtrand.pyx:967\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'a' and 'p' must have same size"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import chess\n",
    "import chess.pgn\n",
    "import random\n",
    "from collections import deque\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load PGN data and other utility functions\n",
    "def load_pgn_data(pgn_file_path):\n",
    "    print(\"Loading PGN data from {}...\".format(pgn_file_path))\n",
    "    pgn_data = []\n",
    "    with open(pgn_file_path) as pgn:\n",
    "        while True:\n",
    "            game = chess.pgn.read_game(pgn)\n",
    "            if game is None:\n",
    "                break\n",
    "            board = game.board()\n",
    "            for move in game.mainline_moves():\n",
    "                input_array = board_to_input_array(board)\n",
    "                output_array = move_to_output_array(move, board.legal_moves)\n",
    "                pgn_data.append((input_array, output_array))\n",
    "                board.push(move)\n",
    "    return pgn_data\n",
    "\n",
    "def board_to_input_array(board):\n",
    "    board_array = np.zeros((8, 8, 12), dtype=np.uint8)\n",
    "    piece_mapping = {'r': 0, 'n': 1, 'b': 2, 'q': 3, 'k': 4, 'p': 5, 'R': 6, 'N': 7, 'B': 8, 'Q': 9, 'K': 10, 'P': 11}\n",
    "    for square, piece in board.piece_map().items():\n",
    "        piece_type = piece_mapping[piece.symbol()]\n",
    "        color = int(piece.color)\n",
    "        board_array[square // 8, square % 8, piece_type] = color + 1\n",
    "    return board_array\n",
    "\n",
    "def move_to_output_array(move, legal_moves):\n",
    "    output_array = np.zeros(action_space_size)\n",
    "    move_index = list(legal_moves).index(move)\n",
    "    output_array[move_index] = 1\n",
    "    return output_array\n",
    "\n",
    "def state_to_index(board):\n",
    "    board_array = np.array(board_to_input_array(board))\n",
    "    return hash(board_array.tostring()) % state_space_size[0]\n",
    "\n",
    "def choose_action(board, model):\n",
    "    if np.random.rand() < exploration_prob:\n",
    "        return np.random.choice(list(board.legal_moves))\n",
    "    else:\n",
    "        state_index = state_to_index(board)\n",
    "        legal_moves_list = list(board.legal_moves)\n",
    "        if not legal_moves_list:\n",
    "            return chess.Move.null()\n",
    "        q_values = model.predict(np.array([board_to_input_array(board)]))[0]\n",
    "        best_move_index = np.argmax(q_values)\n",
    "        best_move_uci = legal_moves_list[min(best_move_index, len(legal_moves_list)-1)].uci()\n",
    "        return chess.Move.from_uci(best_move_uci)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "exploration_prob = 0.2\n",
    "state_space_size = (8, 8, 12)\n",
    "action_space_size = 4096\n",
    "\n",
    "# Initialize a deque for experience replay\n",
    "experience_replay_buffer = deque(maxlen=1000)\n",
    "\n",
    "# Create a policy network\n",
    "def create_policy_network():\n",
    "    input_layer = Input(shape=state_space_size)\n",
    "    conv1 = Conv2D(128, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    flatten_layer = Flatten()(conv2)\n",
    "    dense1 = Dense(128, activation='relu')(flatten_layer)\n",
    "    dense2 = Dense(64, activation='relu')(dense1)\n",
    "    output_probs = Dense(action_space_size, activation='softmax')(dense2)  # Output size matches action_space_size\n",
    "    model = Model(inputs=input_layer, outputs=output_probs)\n",
    "    return model\n",
    "\n",
    "\n",
    "policy_model = create_policy_network()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Function to choose an action using the policy network\n",
    "def choose_policy_action(board, model):\n",
    "    state_array = board_to_input_array(board)\n",
    "    state_array = np.expand_dims(state_array, axis=0)\n",
    "    action_probs = model.predict(state_array)[0]\n",
    "    legal_moves_list = list(board.legal_moves)\n",
    "    action_index = np.random.choice(range(len(legal_moves_list)), p=action_probs)\n",
    "    return legal_moves_list[action_index]\n",
    "\n",
    "# Define a training loop with REINFORCE (Policy Gradient)\n",
    "def train_policy_network(policy_model, episodes):\n",
    "    for episode in range(episodes):\n",
    "        game_states, _, total_reward = play_game()  # Play a game and collect states and rewards\n",
    "        for t, state in enumerate(game_states):\n",
    "            # Compute discounted rewards\n",
    "            discounted_rewards = [total_reward - sum(game_states[i + t:])[2] for i, _ in enumerate(game_states[t:])]\n",
    "            state_array = board_to_input_array(state)\n",
    "            state_array = np.expand_dims(state_array, axis=0)\n",
    "            action_probs = policy_model.predict(state_array)[0]\n",
    "            chosen_action = list(state.legal_moves).index(game_states[t + 1].pop())\n",
    "            log_prob = tf.math.log(action_probs[chosen_action])\n",
    "            loss = -log_prob * discounted_rewards[t]  # REINFORCE loss\n",
    "            gradients = tape.gradient(loss, policy_model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, policy_model.trainable_variables))\n",
    "\n",
    "# Function to play a game\n",
    "def play_game():\n",
    "    board = chess.Board()\n",
    "    game_states = []\n",
    "    total_reward = 0  # Initialize total_reward\n",
    "\n",
    "    while not board.is_game_over():\n",
    "        state = board.copy()\n",
    "        game_states.append(state.copy())\n",
    "\n",
    "        # Choose an action using the policy network\n",
    "        policy_move = choose_policy_action(board, policy_model)\n",
    "\n",
    "        if policy_move in board.legal_moves:\n",
    "            board.push(policy_move)\n",
    "        else:\n",
    "            print(\"Invalid move. Try again.\")\n",
    "            continue\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        if board.is_game_over():\n",
    "            break\n",
    "\n",
    "        stockfish.set_fen_position(board.fen())\n",
    "        stockfish_move_uci = stockfish.get_best_move()\n",
    "        stockfish_move = chess.Move.from_uci(stockfish_move_uci)\n",
    "        next_state = board.copy()\n",
    "        board.push(stockfish_move)\n",
    "\n",
    "        if next_state.is_check():\n",
    "            reward = 0.1\n",
    "\n",
    "        if board.result() == \"1-0\":\n",
    "            reward += 100  # Win\n",
    "        elif board.result() == \"0-1\":\n",
    "            move_number = len(game_states)\n",
    "            reward -= 100  # Loss\n",
    "        elif board.result() == \"1/2-1/2\":\n",
    "            reward -= 100  # Draw\n",
    "\n",
    "        # Capture reward\n",
    "        if board.is_capture(policy_move):\n",
    "            reward += piece_value(board.piece_at(policy_move.to_square))\n",
    "\n",
    "        if board.is_capture(stockfish_move):\n",
    "            reward -= piece_value(board.piece_at(stockfish_move.to_square))\n",
    "\n",
    "        # Update policy network with rewards\n",
    "        discounted_rewards = [total_reward - sum(game_states[i + 1:])[2] for i, _ in enumerate(game_states)]\n",
    "        state_array = board_to_input_array(state)\n",
    "        state_array = np.expand_dims(state_array, axis=0)\n",
    "        action_probs = policy_model.predict(state_array)[0]\n",
    "        chosen_action = list(state.legal_moves).index(policy_move)\n",
    "        log_prob = tf.math.log(action_probs[chosen_action])\n",
    "        loss = -log_prob * discounted_rewards[len(game_states) - 1]  # REINFORCE loss\n",
    "        gradients = tape.gradient(loss, policy_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, policy_model.trainable_variables))\n",
    "\n",
    "        # Accumulate the reward\n",
    "        total_reward += reward\n",
    "\n",
    "    game_states.append(board.copy())\n",
    "    return game_states, board.result(), total_reward\n",
    "\n",
    "\n",
    "log_dir = \"logs/\"\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "num_games = 1\n",
    "total_rewards = []\n",
    "results = {\"white_wins\": 0, \"black_wins\": 0, \"draws\": 0}\n",
    "outcomes = []\n",
    "\n",
    "\n",
    "reward_trend = []\n",
    "\n",
    "for episode in tqdm(range(num_games), desc=\"Training\"):\n",
    "    exploration_prob *= 0.99  # Decay exploration probability\n",
    "    print(\"Game:\", episode)\n",
    "    game_states, result, total_reward = play_game()\n",
    "\n",
    "    # Append the total reward to the reward trend list\n",
    "    reward_trend.append(total_reward)\n",
    "\n",
    "    # Update results based on the game outcome\n",
    "    if result == \"1-0\":\n",
    "        results[\"white_wins\"] += 1\n",
    "        outcomes.append(1)\n",
    "    elif result == \"0-1\":\n",
    "        results[\"black_wins\"] += 1\n",
    "        outcomes.append(0)\n",
    "    elif result == \"1/2-1/2\":\n",
    "        results[\"draws\"] += 1\n",
    "        outcomes.append(0.5)  # Fix here: Append 0.5 for draws\n",
    "\n",
    "    total_rewards.append(total_reward)\n",
    "    \n",
    "    # Display the total reward for each game\n",
    "    print(\"Total Reward for Game {}: {}\".format(episode, total_reward))\n",
    "    print(\"Game Outcome:\", result)\n",
    "    #game length\n",
    "    print(\"Game Length:\", len(game_states))\n",
    "\n",
    "# Display statistics\n",
    "average_reward = sum(total_rewards) / num_games\n",
    "print(\"Average Total Reward:\", average_reward)\n",
    "\n",
    "# Extract the FEN of the final position\n",
    "final_position_fen = game_states[-1].fen()\n",
    "print(\"Final Position FEN:\", final_position_fen)\n",
    "\n",
    "# Display the last game\n",
    "for state in game_states:\n",
    "    display_chess_board(state)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nResults Summary:\")\n",
    "print(\"White Wins:\", results[\"white_wins\"])\n",
    "print(\"Black Wins:\", results[\"black_wins\"])\n",
    "print(\"Draws:\", results[\"draws\"])\n",
    "\n",
    "# Plot trend lines\n",
    "plt.plot(outcomes, label=\"Game Outcomes\")\n",
    "plt.xlabel(\"Games\")\n",
    "plt.ylabel(\"Outcome (1 for White Win, 0 for Draw, 0.5 for Loss)\")\n",
    "plt.legend()\n",
    "plt.title(\"Game Outcomes Trend\")\n",
    "\n",
    "# Plot rewards\n",
    "plt.figure()\n",
    "plt.plot(reward_trend, label=\"Rewards\")\n",
    "plt.xlabel(\"Games\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.legend()\n",
    "plt.title(\"Reward Trend\")\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
